{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfdfe57b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9604075",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created/verified evaluation folder: /net/scratch2/smallyan/erasing-llm_eval/evaluation\n"
     ]
    }
   ],
   "source": [
    "# Create the evaluation folder if it doesn't exist\n",
    "import os\n",
    "repo_path = '/net/scratch2/smallyan/erasing-llm_eval'\n",
    "eval_path = os.path.join(repo_path, 'evaluation')\n",
    "os.makedirs(eval_path, exist_ok=True)\n",
    "print(f\"Created/verified evaluation folder: {eval_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e723f2",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No elm_models folder found\n",
      "\n",
      "Data folder contents:\n",
      "  harrypotter\n",
      "  wmdp-keywords.json\n",
      "  wmdp\n"
     ]
    }
   ],
   "source": [
    "# Check what models are mentioned in the paper's experiments\n",
    "# According to plan.md, the original work used:\n",
    "# - Zephyr-7B (HuggingFaceH4/zephyr-7b-beta)\n",
    "# - Mistral-7B\n",
    "# - Llama3-8B\n",
    "# - Llama3-8B-Instruct \n",
    "# - Qwen2.5-32B\n",
    "# - Llama3-70B\n",
    "# - Llama-2-7B Chat (for Harry Potter experiment)\n",
    "\n",
    "# Let's check if there are trained models in the repo\n",
    "elm_models_path = os.path.join(repo_path, 'elm_models')\n",
    "if os.path.exists(elm_models_path):\n",
    "    print(\"elm_models folder exists:\")\n",
    "    print(os.listdir(elm_models_path))\n",
    "else:\n",
    "    print(\"No elm_models folder found\")\n",
    "\n",
    "# Check the data folder\n",
    "data_path = os.path.join(repo_path, 'data')\n",
    "print(\"\\nData folder contents:\")\n",
    "for item in os.listdir(data_path):\n",
    "    print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd6fa4a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter data:\n",
      "  .ipynb_checkpoints\n",
      "  hp-questions-dual.json\n",
      "  hp-questions.json\n",
      "\n",
      "WMDP data:\n",
      "  bio-questions.json\n",
      "  chem-questions.json\n",
      "  cyber-questions.json\n"
     ]
    }
   ],
   "source": [
    "# Check the data subfolder contents\n",
    "print(\"Harry Potter data:\")\n",
    "hp_path = os.path.join(data_path, 'harrypotter')\n",
    "for item in os.listdir(hp_path):\n",
    "    print(f\"  {item}\")\n",
    "    \n",
    "print(\"\\nWMDP data:\")\n",
    "wmdp_path = os.path.join(data_path, 'wmdp')\n",
    "if os.path.exists(wmdp_path):\n",
    "    for item in os.listdir(wmdp_path):\n",
    "        print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9686259",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebooks folder contents:\n",
      "  inference.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Let's read the notebooks to understand what's being done\n",
    "import json\n",
    "\n",
    "# Read the inference notebook to understand how they test the models\n",
    "notebooks_path = os.path.join(repo_path, 'notebooks')\n",
    "print(\"Notebooks folder contents:\")\n",
    "for item in os.listdir(notebooks_path):\n",
    "    print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f16c00",
   "metadata": {},
   "source": [
    "# Generalizability Evaluation for ELM (Erasing Language Memory)\n",
    "\n",
    "## Overview of the Research\n",
    "\n",
    "This repository implements **ELM (Erasing Language Memory)**, a method for erasing conceptual knowledge from language models. The key aspects are:\n",
    "\n",
    "### Neuron-Level Finding\n",
    "- The method uses **Low-rank adapters (LoRA)** applied to **early model layers (layers 4-7)** to target factual knowledge localization\n",
    "- These specific layers are identified as containing factual knowledge that can be modified without damaging unrelated knowledge\n",
    "- The method leverages the model's introspective classification capabilities to reduce generation probabilities for concept-specific content\n",
    "\n",
    "### Original Models Used\n",
    "- Zephyr-7B (HuggingFaceH4/zephyr-7b-beta)\n",
    "- Mistral-7B\n",
    "- Llama3-8B\n",
    "- Llama3-8B-Instruct\n",
    "- Qwen2.5-32B  \n",
    "- Llama3-70B\n",
    "- Llama-2-7B Chat (for Harry Potter experiment)\n",
    "\n",
    "### Original Datasets\n",
    "- WMDP-Bio (5,000 samples for biosecurity erasure)\n",
    "- WMDP-Cyber (1,000 samples for cybersecurity erasure)\n",
    "- Harry Potter texts (3,000 samples)\n",
    "- Retain datasets from safe concepts\n",
    "\n",
    "### Method Components\n",
    "1. **Lerase**: Cross-entropy between ELM model and classifier-modified distribution\n",
    "2. **Lretain**: Preserve behavior on safe concepts\n",
    "3. **Lfluency**: Maintain coherent generation for smaller models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b48e6bc",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA H100 NVL\n",
      "GPU Memory: 99.95 GB\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b862954",
   "metadata": {},
   "source": [
    "## GT1: Generalization to a New Model\n",
    "\n",
    "**Objective**: Test if the neuron-level finding (early layers 4-7 contain factual knowledge that can be erased via LoRA) generalizes to a model NOT used in the original work.\n",
    "\n",
    "**Original models used**: Zephyr-7B, Mistral-7B, Llama3-8B, Llama3-8B-Instruct, Qwen2.5-32B, Llama3-70B, Llama-2-7B Chat\n",
    "\n",
    "**New model to test**: We'll use **Gemma-2B** (google/gemma-2b) which is:\n",
    "1. Not in the original paper\n",
    "2. Has a different architecture (different layer structure)\n",
    "3. Small enough to run efficiently\n",
    "\n",
    "We will test if training LoRA on early layers of Gemma-2B can erase Harry Potter knowledge using the same ELM methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ec4f4d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# First, let's understand what models are available and what the finding actually is\n",
    "# The key finding: Early layers (4-7) for a 7B model contain factual knowledge\n",
    "# For smaller models, we need to scale the layer range proportionally\n",
    "\n",
    "# Let's set up the environment\n",
    "import sys\n",
    "sys.path.append('/net/scratch2/smallyan/erasing-llm_eval')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "device = 'cuda:0'\n",
    "dtype = torch.bfloat16  # Use bfloat16 for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "720f3956",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GT1: Model Generalization\n",
      "==================================================\n",
      "New model to test: microsoft/phi-2\n",
      "This model was NOT used in the original ELM paper\n",
      "\n",
      "Loading model: microsoft/phi-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6946c1d167dc4650b4b650deec69b87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a0e240a84a492f96d124f199c540b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Number of layers: 32\n"
     ]
    }
   ],
   "source": [
    "# For GT1, we need to test if the ELM method works on a NEW model not used in the original paper\n",
    "# The key neuron-level finding is: \"Early layers (4-7) contain factual knowledge that can be erased via LoRA\"\n",
    "\n",
    "# Original models used: Zephyr-7B, Mistral-7B, Llama3-8B, Llama3-8B-Instruct, Qwen2.5-32B, Llama3-70B, Llama-2-7B Chat\n",
    "# All are based on Llama or Mistral architecture\n",
    "\n",
    "# We'll test with Phi-2 (microsoft/phi-2) - a 2.7B model with different architecture\n",
    "# It was not used in the original paper\n",
    "\n",
    "print(\"Testing GT1: Model Generalization\")\n",
    "print(\"=\" * 50)\n",
    "print(\"New model to test: microsoft/phi-2\")\n",
    "print(\"This model was NOT used in the original ELM paper\")\n",
    "print()\n",
    "\n",
    "# Load the new model\n",
    "model_id = \"microsoft/phi-2\"\n",
    "print(f\"Loading model: {model_id}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = model.to(device)\n",
    "model.requires_grad_(False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Number of layers: {len(model.model.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7be3260f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE ELM - Testing Harry Potter knowledge:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Harry Potter's best friends are\n",
      "Response: Harry Potter's best friends are Ron Weasley and Hermione Granger.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The headmaster of Hogwarts is\n",
      "Response: The headmaster of Hogwarts is Albus Dumbledore.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The game played on broomsticks at Hogwarts is called\n",
      "Response: The game played on broomsticks at Hogwarts is called Quidditch.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# First, let's test that the model knows about Harry Potter before any modification\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test Harry Potter knowledge before ELM\n",
    "test_prompts_hp = [\n",
    "    \"Harry Potter's best friends are\",\n",
    "    \"The headmaster of Hogwarts is\",\n",
    "    \"The game played on broomsticks at Hogwarts is called\"\n",
    "]\n",
    "\n",
    "print(\"BEFORE ELM - Testing Harry Potter knowledge:\")\n",
    "print(\"=\" * 50)\n",
    "for prompt in test_prompts_hp:\n",
    "    response = generate_text(model, tokenizer, prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a793879b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peft'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now let's apply the ELM method to Phi-2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The key finding: early layers contain factual knowledge\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# For a 32-layer model like Phi-2, we'll use layers 4-7 (same as 7B models which also have 32 layers)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KLDivLoss\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'"
     ]
    }
   ],
   "source": [
    "# Now let's apply the ELM method to Phi-2\n",
    "# The key finding: early layers contain factual knowledge\n",
    "# For a 32-layer model like Phi-2, we'll use layers 4-7 (same as 7B models which also have 32 layers)\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import KLDivLoss\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# ELM Parameters (same as original paper)\n",
    "lora_layer_start = 4\n",
    "lora_layer_end = 8\n",
    "rank = 4\n",
    "alpha = 16\n",
    "eta = 500\n",
    "\n",
    "# Define LoRA configuration targeting early layers\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"fc1\", \"fc2\"]  # Phi-2 uses fc1/fc2 for MLP\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=rank,\n",
    "    lora_alpha=alpha,\n",
    "    layers_to_transform=list(range(lora_layer_start, lora_layer_end)),\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Add LoRA to model\n",
    "model_peft = get_peft_model(model, lora_config)\n",
    "model_peft.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33077cd5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install peft\n",
    "import subprocess\n",
    "subprocess.run(['pip', 'install', 'peft', '-q'], check=True)\n",
    "print(\"peft installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "197bd112",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.modeling_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now let's apply the ELM method to Phi-2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KLDivLoss\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/peft/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.18.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[1;32m     19\u001b[0m     AutoPeftModel,\n\u001b[1;32m     20\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[1;32m     21\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[1;32m     22\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[1;32m     23\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[1;32m     24\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[1;32m     25\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig, PromptLearningConfig\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001b[1;32m     30\u001b[0m     PEFT_TYPE_TO_MIXED_MODEL_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     inject_adapter_in_model,\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/peft/auto.py:32\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     AutoModel,\n\u001b[1;32m     23\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     AutoTokenizer,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     PeftModel,\n\u001b[1;32m     34\u001b[0m     PeftModelForCausalLM,\n\u001b[1;32m     35\u001b[0m     PeftModelForFeatureExtraction,\n\u001b[1;32m     36\u001b[0m     PeftModelForQuestionAnswering,\n\u001b[1;32m     37\u001b[0m     PeftModelForSeq2SeqLM,\n\u001b[1;32m     38\u001b[0m     PeftModelForSequenceClassification,\n\u001b[1;32m     39\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mother\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_file_exists_on_hf_hub\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/peft/peft_model.py:42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuestionAnsweringModelOutput, SequenceClassifierOutput, TokenClassifierOutput\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_alora_offsets_for_forward, get_alora_offsets_for_generate\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTuner, BaseTunerLayer\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AuxiliaryTrainingWrapper\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/peft/tuners/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madalora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaLoraConfig, AdaLoraModel\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madaption_prompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaptionPromptConfig, AdaptionPromptModel\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BOFTConfig, BOFTModel\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/peft/tuners/adalora/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bnb_4bit_available, is_bnb_available\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_peft_method\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaLoraConfig\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgptq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVDQuantLinear\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaLoraLayer, RankAllocator, SVDLinear\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/peft/tuners/adalora/config.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass, field\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftType\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAdaLoraConfig\u001b[39;00m(LoraConfig):\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/peft/tuners/lora/__init__.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgptq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPTQLoraLinear\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2d, Conv3d, Embedding, Linear, LoraLayer, ParamWrapper\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraModel\n\u001b[1;32m     26\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArrowConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConv2d\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitialize_lora_eva_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     43\u001b[0m ]\n\u001b[1;32m     45\u001b[0m register_peft_method(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlora\u001b[39m\u001b[38;5;124m\"\u001b[39m, config_cls\u001b[38;5;241m=\u001b[39mLoraConfig, model_cls\u001b[38;5;241m=\u001b[39mLoraModel, is_mixed_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/peft/tuners/lora/model.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_layers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradientCheckpointingLayer\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bnb_4bit_available, is_bnb_available\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     BaseTuner,\n\u001b[1;32m     31\u001b[0m     BaseTunerLayer,\n\u001b[1;32m     32\u001b[0m     replicate_layers,\n\u001b[1;32m     33\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling_layers'"
     ]
    }
   ],
   "source": [
    "# Now let's apply the ELM method to Phi-2\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import KLDivLoss\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# ELM Parameters (same as original paper)\n",
    "lora_layer_start = 4\n",
    "lora_layer_end = 8\n",
    "rank = 4\n",
    "alpha = 16\n",
    "eta = 500\n",
    "\n",
    "# Define LoRA configuration targeting early layers\n",
    "# Phi-2 uses different naming conventions\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\"]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=rank,\n",
    "    lora_alpha=alpha,\n",
    "    layers_to_transform=list(range(lora_layer_start, lora_layer_end)),\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Add LoRA to model\n",
    "model_peft = get_peft_model(model, lora_config)\n",
    "model_peft.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b94607",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft 0.11.1 installed successfully\n"
     ]
    }
   ],
   "source": [
    "# There's a version conflict. Let's use a compatible version\n",
    "import subprocess\n",
    "subprocess.run(['pip', 'install', 'peft==0.11.1', '-q'], check=True)\n",
    "print(\"peft 0.11.1 installed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-12-23-02-36_GeneralizationEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
