{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac9ac6e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d94a2",
   "metadata": {},
   "source": [
    "# Code Evaluation for Circuit Analysis Project\n",
    "\n",
    "## Project Overview\n",
    "This project analyzes concept and token induction heads in Llama-2-7b to enable parallelogram arithmetic (e.g., Athens â€“ Greece + China = Beijing) using word2vec-style analogies.\n",
    "\n",
    "## Repository: `/net/scratch2/smallyan/arithmetic_eval`\n",
    "\n",
    "## Files to Evaluate (per CodeWalkthrough.md):\n",
    "1. `scripts/parallelograms.py` - Helper functions for analysis\n",
    "2. `scripts/all_parallelograms.py` - Main analysis script for all tasks\n",
    "3. `scripts/parallelogram_ranks.py` - Low-rank approximation analysis\n",
    "4. `scripts/parallelogram_analysis.ipynb` - Plotting code for figures\n",
    "\n",
    "## Evaluation Criteria:\n",
    "- **Runnable (Y/N)**: Executes without error\n",
    "- **Correct-Implementation (Y/N)**: Logic matches described computation\n",
    "- **Redundant (Y/N)**: Duplicates another block's computation\n",
    "- **Irrelevant (Y/N)**: Does not contribute to project goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600df2da",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100 80GB PCIe\n",
      "CUDA devices count: 1\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA devices count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b1ccfd",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /net/scratch2/smallyan/arithmetic_eval/scripts\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "import sys\n",
    "sys.path.insert(0, '/net/scratch2/smallyan/arithmetic_eval/scripts')\n",
    "REPO_PATH = '/net/scratch2/smallyan/arithmetic_eval'\n",
    "\n",
    "# Change to scripts directory for relative path references\n",
    "import os\n",
    "os.chdir(f'{REPO_PATH}/scripts')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30024690",
   "metadata": {},
   "source": [
    "## 1. Evaluating `parallelograms.py` Functions\n",
    "\n",
    "This file contains the core helper functions for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f434abbb",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 1 (imports): SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Import statements from parallelograms.py\n",
    "try:\n",
    "    import os \n",
    "    import torch \n",
    "    import json \n",
    "    import argparse\n",
    "    import matplotlib.pyplot as plt \n",
    "    from nnsight import LanguageModel\n",
    "    \n",
    "    block1_result = {\n",
    "        \"block_id\": \"parallelograms.py:imports\",\n",
    "        \"runnable\": True,\n",
    "        \"error_note\": None\n",
    "    }\n",
    "    print(\"Block 1 (imports): SUCCESS\")\n",
    "except Exception as e:\n",
    "    block1_result = {\n",
    "        \"block_id\": \"parallelograms.py:imports\",\n",
    "        \"runnable\": False,\n",
    "        \"error_note\": str(e)\n",
    "    }\n",
    "    print(f\"Block 1 (imports): FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad82a24b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocks 2-4 (logit_lens, print_logit_lens, proj_onto_ov): Function definitions SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 2: logit_lens function\n",
    "def logit_lens(concept_vec, model):\n",
    "    with torch.no_grad():\n",
    "        return model.lm_head(model.model.norm(concept_vec.cuda())).softmax(dim=-1).detach().cpu()\n",
    "\n",
    "# Block 3: print_logit_lens function\n",
    "def print_logit_lens(probs, tokenizer, label=''):\n",
    "    topprobs, idxs = torch.topk(probs, k=10)\n",
    "    print(f'{label} logit lens\\t', [(tokenizer.decode(t), round(p.item(), 3)) for t, p in zip(idxs, topprobs)])\n",
    "\n",
    "# Block 4: proj_onto_ov function\n",
    "def proj_onto_ov(w, ov_sum, model, layer_idx, head_ordering='concept', offset=-1, w_prefix=''):\n",
    "    w = w_prefix + w.strip()\n",
    "    if head_ordering == 'raw':\n",
    "        with torch.no_grad(), model.trace(w):\n",
    "            state = model.model.layers[layer_idx].output[0].squeeze()[offset].save()\n",
    "        return state \n",
    "    with torch.no_grad():\n",
    "        with model.trace(w):\n",
    "            state = model.model.layers[layer_idx].output[0].squeeze()[offset].detach().save()\n",
    "    return torch.matmul(ov_sum, state)\n",
    "\n",
    "print(\"Blocks 2-4 (logit_lens, print_logit_lens, proj_onto_ov): Function definitions SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c404e23",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 5 (get_ov_sum): Function definition SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 5: get_ov_sum function\n",
    "def get_ov_sum(model, head_ordering='concept', k=80, rank=4096):\n",
    "    head_dim = model.config.hidden_size // model.config.num_attention_heads\n",
    "    model_name = model.config._name_or_path.split('/')[-1]\n",
    "    \n",
    "    if head_ordering == 'raw':\n",
    "        return None\n",
    "    elif head_ordering == 'all':\n",
    "        to_sum = [(l, h) for l in range(model.config.num_hidden_layers) for h in range(model.config.num_attention_heads)]\n",
    "    else: \n",
    "        with open(f'../cache/causal_scores/{model_name}/{head_ordering}_copying_len30_n1024.json', 'r') as f: \n",
    "            temp = json.load(f)\n",
    "        tups = sorted([(d['layer'], d['head_idx'], d['score']) for d in temp], key=lambda t: t[2], reverse=True)\n",
    "        to_sum = [(l, h) for l, h, _ in tups][:k]\n",
    "    layerset = set([l for l, _ in to_sum])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ov_sum = torch.zeros((4096, 4096), device='cuda')\n",
    "        for layer in layerset:\n",
    "            for l, h in to_sum:\n",
    "                if l == layer:\n",
    "                    V = model.model.layers[l].self_attn.v_proj.weight[h * head_dim : (h+1) * head_dim]\n",
    "                    O = model.model.layers[l].self_attn.o_proj.weight[:, h * head_dim : (h+1) * head_dim]\n",
    "                    ov_sum += torch.matmul(O, V)\n",
    "        \n",
    "        if rank < model.config.hidden_size:\n",
    "            U, S, Vh = torch.linalg.svd(ov_sum)\n",
    "            ov_sum = (U[:, :rank] * S[:rank]) @ Vh[:rank]\n",
    "        return ov_sum\n",
    "\n",
    "print(\"Block 5 (get_ov_sum): Function definition SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e3394a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 6 (get_neighbors): Function definition SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 6: get_neighbors function\n",
    "def get_neighbors(task_lines, model, layer, head_ordering, k, w_prefixes, dataset, rank):\n",
    "    sep = ' ' if dataset == 'word2vec' else '\\t'\n",
    "    ov_sum = get_ov_sum(model, head_ordering, k, rank)\n",
    "\n",
    "    if w_prefixes[0] == w_prefixes[1]:\n",
    "        neighbors = set([w for l in task_lines for w in l.split(sep)])\n",
    "        neighbors = {\n",
    "            w : proj_onto_ov(w, ov_sum, model, layer, head_ordering=head_ordering, w_prefix=w_prefixes[0])\n",
    "            for w in neighbors  \n",
    "        }\n",
    "    else: \n",
    "        left_neighbors = set([l.split(sep)[0] for l in task_lines])\n",
    "        right_neighbors = set([l.split(sep)[1] for l in task_lines])\n",
    "        neighbors = {}\n",
    "        for w in left_neighbors:\n",
    "            neighbors[w] = proj_onto_ov(w, model, layer, head_ordering=head_ordering, k=k, w_prefix=w_prefixes[0])\n",
    "        for w in right_neighbors:\n",
    "            neighbors[w] = proj_onto_ov(w, model, layer, head_ordering=head_ordering, k=k, w_prefix=w_prefixes[1])\n",
    "\n",
    "    return neighbors\n",
    "\n",
    "print(\"Block 6 (get_neighbors): Function definition SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad08a83",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 7 (get_parallelogram_scores): Function definition SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 7: get_parallelogram_scores function\n",
    "def get_parallelogram_scores(a, b, c, d, neighbors, model, verbose=False):\n",
    "    aw, bw, cw, dw = a, b, c, d\n",
    "    a = neighbors[aw]\n",
    "    b = neighbors[bw]\n",
    "    c = neighbors[cw]\n",
    "    d = neighbors[dw]\n",
    "\n",
    "    ans_tok = model.tokenizer(cw)['input_ids'][1]\n",
    "    ans_str = model.tokenizer.decode(ans_tok)\n",
    "\n",
    "    probs = logit_lens((a - b) + d, model)\n",
    "    pred = model.tokenizer.decode(probs.argmax(dim=-1))\n",
    "\n",
    "    ll_correct = pred.strip().lower() == ans_str.strip().lower()\n",
    "    ll_pans = probs[ans_tok].item()\n",
    "\n",
    "    admean = (a + d) / 2\n",
    "    bcmean = (b + c) / 2\n",
    "    score = torch.norm(admean - bcmean) / (torch.norm(a - d) + torch.norm(b - c))\n",
    "    \n",
    "    similarities = {}\n",
    "    for k in neighbors.keys():\n",
    "        similarities[k] = torch.cosine_similarity((a - b) + d, neighbors[k], dim=0)\n",
    "    nn_correct = max(similarities, key=similarities.get) == cw        \n",
    "    if verbose:\n",
    "        print(f'{aw} - {bw} + {dw} : {cw}?', pred, ll_correct, f'parallel_score={round(score.item(), 3)}') \n",
    "        print('neighbors:', sorted(similarities, key=similarities.get, reverse=True)[:5])\n",
    "\n",
    "    return ll_correct, ll_pans, score.item(), nn_correct\n",
    "\n",
    "print(\"Block 7 (get_parallelogram_scores): Function definition SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7722b25e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 8 (all_dot_products): Function definition SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 8: all_dot_products function\n",
    "def all_dot_products(task_lines, neighbors, model, k, head_ordering, dataset, task_name, layer, w_prefixes, rank):\n",
    "    sep = ' ' if dataset == 'word2vec' else '\\t'\n",
    "    dots = []\n",
    "    cosines = []\n",
    "    for line in task_lines:\n",
    "        if len(line.split(sep)) == 4:\n",
    "            a, b, aprime, bprime = line.split(sep)\n",
    "            a = neighbors[a]\n",
    "            b = neighbors[b]\n",
    "            aprime = neighbors[aprime]\n",
    "            bprime = neighbors[bprime]\n",
    "\n",
    "            dots.append(torch.dot(a - b, aprime - bprime).item())\n",
    "            cosines.append(torch.cosine_similarity(a - b, aprime - bprime, dim=0).item())\n",
    "\n",
    "    if w_prefixes[0] == '' and w_prefixes[1] == '':\n",
    "        superfolder = 'no_prefix'\n",
    "    else:\n",
    "        superfolder = 'with_prefix'\n",
    "    os.makedirs(f'../cache/parallelograms/{dataset}/{superfolder}/{head_ordering}/{task_name}', exist_ok=True)\n",
    "    os.makedirs(f'../figures/parallelograms/{dataset}/{superfolder}/{task_name}', exist_ok=True)\n",
    "\n",
    "    fname = f'layer{layer}'\n",
    "    fname += f'_rank{rank}' if rank < model.config.hidden_size else ''\n",
    "\n",
    "    results = {\n",
    "        'dots' : dots,\n",
    "        'cosines' : cosines \n",
    "    }\n",
    "    with open(f'../cache/parallelograms/{dataset}/{superfolder}/{head_ordering}/{task_name}/{fname}_dots.json', 'w') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    colors = {\n",
    "        'all' : 'green',\n",
    "        'concept' : 'indianred',\n",
    "        'token' : 'cornflowerblue',\n",
    "        'raw' : 'tab:orange'\n",
    "    }\n",
    "    plt.hist(dots, color=colors[head_ordering], edgecolor='black')\n",
    "    plt.title(f'All Possible {task_name} Dot Products')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Dot Product of Diff. Pair (e.g. (man - woman) * (king - queen))')\n",
    "    plt.savefig(f'../figures/parallelograms/{dataset}/{superfolder}/{task_name}/{head_ordering}_{fname}_dot_hist.png')\n",
    "    plt.clf()\n",
    "\n",
    "    plt.hist(cosines, color=colors[head_ordering], edgecolor='black')\n",
    "    plt.title(f'All Possible {task_name} Cosine Similarities')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Cosine Sim. of Diff. Pair (e.g. (man - woman) * (king - queen))')\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.savefig(f'../figures/parallelograms/{dataset}/{superfolder}/{task_name}/{head_ordering}_{fname}_cosine_hist.png')\n",
    "    plt.clf()\n",
    "\n",
    "print(\"Block 8 (all_dot_products): Function definition SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3542d5ac",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 9 (calculate_save_scores): Function definition SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 9: calculate_save_scores function\n",
    "def calculate_save_scores(task_lines, neighbors, model, k, head_ordering, dataset, task_name, layer, w_prefixes, rank):\n",
    "    sep = ' ' if dataset == 'word2vec' else '\\t'\n",
    "    ll_acc = 0; n = 0\n",
    "    panswers = []\n",
    "    parallelogram_scores = []\n",
    "    nn_acc = 0 \n",
    "    for line in task_lines:\n",
    "        if len(line.split(sep)) == 4:\n",
    "            a, b, aprime, bprime = line.split(sep)\n",
    "            ll_corr, ll_pans, score, nn_corr = get_parallelogram_scores(\n",
    "                a, b, aprime, bprime, neighbors, model, verbose=False\n",
    "            ) \n",
    "            ll_acc += ll_corr \n",
    "            n += 1 \n",
    "            panswers.append(ll_pans)\n",
    "            parallelogram_scores.append(score)\n",
    "            nn_acc += nn_corr\n",
    "\n",
    "    ll_acc /= n\n",
    "    nn_acc /= n \n",
    "    print(head_ordering, task_name, 'layer', layer)\n",
    "    print('logit lens accuracy', ll_acc)\n",
    "    print('nearest neighbor accuracy', nn_acc)\n",
    "    print('average P(aprime)', sum(panswers) / len(panswers))\n",
    "    print('average parallelogram score', sum(parallelogram_scores) / len(parallelogram_scores))\n",
    "\n",
    "    results = {\n",
    "        'll_acc' : ll_acc,\n",
    "        'nn_acc' : nn_acc,\n",
    "        'n' : n,\n",
    "        'll_panswers' : panswers,\n",
    "        'parallelogram_scores' : parallelogram_scores,\n",
    "    }\n",
    "\n",
    "    if w_prefixes[0] == '' and w_prefixes[1] == '':\n",
    "        superfolder = 'no_prefix'\n",
    "    else:\n",
    "        superfolder = 'with_prefix'\n",
    "    os.makedirs(f'../cache/parallelograms/{dataset}/{superfolder}/{head_ordering}/{task_name}', exist_ok=True)\n",
    "\n",
    "    fname = f'layer{layer}'\n",
    "    fname += f'_rank{rank}' if rank < model.config.hidden_size else ''\n",
    "    fname += '_results.json'\n",
    "\n",
    "    with open(f'../cache/parallelograms/{dataset}/{superfolder}/{head_ordering}/{task_name}/{fname}', 'w') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "print(\"Block 9 (calculate_save_scores): Function definition SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd4ab1bd",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama-2-7b model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc260e59be34ddfb79c9e97a6e65b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b154b9ed2c452b8ea18fcab6e81648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0311fe45273241ee9e0ef4fbc0a3617a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9a11a63d594dbd8505d0a35a1747d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02b776e5ab84f69b57b3d16ba0e668e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa6732371c8486286487f4912315078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c13a88b80e4b6c882944ba809976c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade97cde129a4cf88413a151532ae2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb543c6bea644ca8f553ff0fd5ffc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cad100555a84674b1ae33a8ae38fcf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6d78eec29d45d5986680b5c1774fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Now let's load the model and test the functions actually work\n",
    "print(\"Loading Llama-2-7b model...\")\n",
    "model = LanguageModel('meta-llama/Llama-2-7b-hf', device_map='cuda', dispatch=True)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d8365f5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing get_ov_sum with 'concept' head ordering...\n",
      "OV sum shape: torch.Size([4096, 4096])\n",
      "Block 5 (get_ov_sum - concept): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Test Block 5: get_ov_sum with concept ordering\n",
    "print(\"Testing get_ov_sum with 'concept' head ordering...\")\n",
    "try:\n",
    "    ov_sum_concept = get_ov_sum(model, head_ordering='concept', k=80, rank=4096)\n",
    "    print(f\"OV sum shape: {ov_sum_concept.shape}\")\n",
    "    block5_test_concept = True\n",
    "    print(\"Block 5 (get_ov_sum - concept): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block5_test_concept = False\n",
    "    print(f\"Block 5 (get_ov_sum - concept): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38f1253b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing get_ov_sum with 'token' head ordering...\n",
      "OV sum shape: torch.Size([4096, 4096])\n",
      "Block 5 (get_ov_sum - token): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Test Block 5: get_ov_sum with token ordering\n",
    "print(\"Testing get_ov_sum with 'token' head ordering...\")\n",
    "try:\n",
    "    ov_sum_token = get_ov_sum(model, head_ordering='token', k=80, rank=4096)\n",
    "    print(f\"OV sum shape: {ov_sum_token.shape}\")\n",
    "    block5_test_token = True\n",
    "    print(\"Block 5 (get_ov_sum - token): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block5_test_token = False\n",
    "    print(f\"Block 5 (get_ov_sum - token): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a624e8e9",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing get_ov_sum with 'all' head ordering...\n",
      "OV sum shape: torch.Size([4096, 4096])\n",
      "Block 5 (get_ov_sum - all): RUNTIME TEST SUCCESS\n",
      "\n",
      "Testing get_ov_sum with 'raw' head ordering...\n",
      "OV sum for raw: None\n",
      "Block 5 (get_ov_sum - raw): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Test Block 5: get_ov_sum with 'all' and 'raw' orderings\n",
    "print(\"Testing get_ov_sum with 'all' head ordering...\")\n",
    "try:\n",
    "    ov_sum_all = get_ov_sum(model, head_ordering='all', k=80, rank=4096)\n",
    "    print(f\"OV sum shape: {ov_sum_all.shape}\")\n",
    "    block5_test_all = True\n",
    "    print(\"Block 5 (get_ov_sum - all): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block5_test_all = False\n",
    "    print(f\"Block 5 (get_ov_sum - all): RUNTIME TEST FAILED - {e}\")\n",
    "\n",
    "print(\"\\nTesting get_ov_sum with 'raw' head ordering...\")\n",
    "try:\n",
    "    ov_sum_raw = get_ov_sum(model, head_ordering='raw', k=80, rank=4096)\n",
    "    print(f\"OV sum for raw: {ov_sum_raw}\")  # Should be None\n",
    "    block5_test_raw = True\n",
    "    print(\"Block 5 (get_ov_sum - raw): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block5_test_raw = False\n",
    "    print(f\"Block 5 (get_ov_sum - raw): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "710e113d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sample task data (capital-common-countries)...\n",
      "Loaded 506 examples\n",
      "Sample line: Athens Greece Baghdad Iraq\n"
     ]
    }
   ],
   "source": [
    "# Load a sample task to test the full pipeline\n",
    "print(\"Loading sample task data (capital-common-countries)...\")\n",
    "with open('../data/word2vec/capital-common-countries.txt', 'r') as f:\n",
    "    stuff = f.read()\n",
    "sample_task = [l for l in stuff.split('\\n')[1:] if l != '']\n",
    "print(f\"Loaded {len(sample_task)} examples\")\n",
    "print(f\"Sample line: {sample_task[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96defdad",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing get_neighbors function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 12 neighbor representations\n",
      "Sample keys: ['China', 'Bern', 'Beijing', 'Athens', 'Baghdad']\n",
      "Block 6 (get_neighbors): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Test Block 6: get_neighbors function\n",
    "print(\"Testing get_neighbors function...\")\n",
    "try:\n",
    "    # Use only first 5 lines for quick test\n",
    "    test_lines = sample_task[:5]\n",
    "    neighbors = get_neighbors(test_lines, model, layer=20, head_ordering='concept', k=80, \n",
    "                              w_prefixes=('', ''), dataset='word2vec', rank=4096)\n",
    "    print(f\"Got {len(neighbors)} neighbor representations\")\n",
    "    print(f\"Sample keys: {list(neighbors.keys())[:5]}\")\n",
    "    block6_test = True\n",
    "    print(\"Block 6 (get_neighbors): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block6_test = False\n",
    "    print(f\"Block 6 (get_neighbors): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81227044",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing get_parallelogram_scores function...\n",
      "Block 7 (get_parallelogram_scores): RUNTIME TEST FAILED - 'NoneType' object has no attribute 'module_proxy'\n"
     ]
    }
   ],
   "source": [
    "# Test Block 7: get_parallelogram_scores function\n",
    "print(\"Testing get_parallelogram_scores function...\")\n",
    "try:\n",
    "    # Test with Athens - Greece + Iraq = Baghdad\n",
    "    ll_correct, ll_pans, score, nn_correct = get_parallelogram_scores(\n",
    "        'Athens', 'Greece', 'Baghdad', 'Iraq', neighbors, model, verbose=True\n",
    "    )\n",
    "    print(f\"\\nResults: ll_correct={ll_correct}, ll_pans={ll_pans:.4f}, score={score:.4f}, nn_correct={nn_correct}\")\n",
    "    block7_test = True\n",
    "    print(\"Block 7 (get_parallelogram_scores): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block7_test = False\n",
    "    print(f\"Block 7 (get_parallelogram_scores): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9d8e798",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking neighbors structure:\n",
      "  China: type=<class 'torch.Tensor'>, shape=torch.Size([4096])\n",
      "  Bern: type=<class 'torch.Tensor'>, shape=torch.Size([4096])\n",
      "  Beijing: type=<class 'torch.Tensor'>, shape=torch.Size([4096])\n",
      "  Athens: type=<class 'torch.Tensor'>, shape=torch.Size([4096])\n",
      "  Baghdad: type=<class 'torch.Tensor'>, shape=torch.Size([4096])\n",
      "  Switzerland: type=<class 'torch.Tensor'>, shape=torch.Size([4096])\n",
      "  Berlin: type=<class 'torch.Tensor'>, shape=torch.Size([4096])\n",
      "  Bangkok: type=<class 'torch.Tensor'>, shape=torch.Size([4096])\n",
      "  Greece: type=<class 'torch.Tensor'>, shape=torch.Size([4096])\n",
      "  Germany: type=<class 'torch.Tensor'>, shape=torch.Size([4096])\n",
      "  Thailand: type=<class 'torch.Tensor'>, shape=torch.Size([4096])\n",
      "  Iraq: type=<class 'torch.Tensor'>, shape=torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "# Let's check what's in neighbors\n",
    "print(\"Checking neighbors structure:\")\n",
    "for k, v in neighbors.items():\n",
    "    print(f\"  {k}: type={type(v)}, shape={v.shape if hasattr(v, 'shape') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5ec105d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model structure for logit_lens...\n",
      "model.lm_head: Linear(in_features=4096, out_features=32000, bias=False)\n",
      "model.model.norm: LlamaRMSNorm((4096,), eps=1e-05)\n"
     ]
    }
   ],
   "source": [
    "# The issue is with logit_lens - let's check model structure\n",
    "print(\"Checking model structure for logit_lens...\")\n",
    "print(f\"model.lm_head: {model.lm_head}\")\n",
    "print(f\"model.model.norm: {model.model.norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "209ca39e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing logit_lens function...\n",
      "Input vector shape: torch.Size([4096])\n",
      "Block 2 (logit_lens): RUNTIME TEST FAILED - 'NoneType' object has no attribute 'module_proxy'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_820772/596449857.py\", line 6, in <module>\n",
      "    probs = logit_lens(test_vec, model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_820772/2143462025.py\", line 4, in logit_lens\n",
      "    return model.lm_head(model.model.norm(concept_vec.cuda())).softmax(dim=-1).detach().cpu()\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/smallyan/.conda/envs/meta/lib/python3.11/site-packages/nnsight/envoy.py\", line 396, in __call__\n",
      "    module_proxy = getattr(self._tracer._graph.module_proxy, self._module_path)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'module_proxy'\n"
     ]
    }
   ],
   "source": [
    "# The problem is that the model uses nnsight proxies. Let's test logit_lens directly\n",
    "print(\"Testing logit_lens function...\")\n",
    "try:\n",
    "    test_vec = neighbors['Athens']\n",
    "    print(f\"Input vector shape: {test_vec.shape}\")\n",
    "    probs = logit_lens(test_vec, model)\n",
    "    print(f\"Output probs shape: {probs.shape}\")\n",
    "    block2_test = True\n",
    "    print(\"Block 2 (logit_lens): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block2_test = False\n",
    "    print(f\"Block 2 (logit_lens): RUNTIME TEST FAILED - {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d587a8fb",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking proper model access...\n",
      "Type of model: <class 'nnsight.models.LanguageModel.LanguageModel'>\n",
      "model._model: True\n",
      "underlying_model type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "underlying_model.lm_head: Linear(in_features=4096, out_features=32000, bias=False)\n",
      "underlying_model.model.norm: LlamaRMSNorm((4096,), eps=1e-05)\n"
     ]
    }
   ],
   "source": [
    "# The issue is that nnsight model requires using the underlying model for direct operations\n",
    "# Let's check how to properly access lm_head and norm\n",
    "print(\"Checking proper model access...\")\n",
    "print(f\"Type of model: {type(model)}\")\n",
    "print(f\"model._model: {hasattr(model, '_model')}\")\n",
    "\n",
    "# Access the underlying PyTorch model\n",
    "underlying_model = model._model\n",
    "print(f\"underlying_model type: {type(underlying_model)}\")\n",
    "print(f\"underlying_model.lm_head: {underlying_model.lm_head}\")\n",
    "print(f\"underlying_model.model.norm: {underlying_model.model.norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7ab8d7f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fixed logit_lens function...\n",
      "Output probs shape: torch.Size([32000])\n",
      "Top prediction: Greece\n",
      "Block 2 (logit_lens - FIXED): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# The original code has a bug when using nnsight model - need to access _model\n",
    "# Let's redefine logit_lens to use the proper model access\n",
    "def logit_lens_fixed(concept_vec, model):\n",
    "    with torch.no_grad():\n",
    "        # Use the underlying model for direct operations\n",
    "        underlying = model._model\n",
    "        return underlying.lm_head(underlying.model.norm(concept_vec.cuda())).softmax(dim=-1).detach().cpu()\n",
    "\n",
    "# Test the fixed version\n",
    "print(\"Testing fixed logit_lens function...\")\n",
    "try:\n",
    "    test_vec = neighbors['Athens']\n",
    "    probs = logit_lens_fixed(test_vec, model)\n",
    "    print(f\"Output probs shape: {probs.shape}\")\n",
    "    print(f\"Top prediction: {model.tokenizer.decode(probs.argmax(dim=-1))}\")\n",
    "    block2_test_fixed = True\n",
    "    print(\"Block 2 (logit_lens - FIXED): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block2_test_fixed = False\n",
    "    print(f\"Block 2 (logit_lens - FIXED): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01e361a0",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fixed get_parallelogram_scores...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athens - Greece + Iraq : Baghdad? Ira False parallel_score=0.138\n",
      "neighbors: ['Baghdad', 'Iraq', 'Athens', 'Bangkok', 'Beijing']\n",
      "\n",
      "Results: ll_correct=False, ll_pans=0.0169, score=0.1377, nn_correct=True\n",
      "Block 7 (get_parallelogram_scores - FIXED): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Now let's redefine get_parallelogram_scores with the fix and test it\n",
    "def get_parallelogram_scores_fixed(a, b, c, d, neighbors, model, verbose=False):\n",
    "    aw, bw, cw, dw = a, b, c, d\n",
    "    a = neighbors[aw]\n",
    "    b = neighbors[bw]\n",
    "    c = neighbors[cw]\n",
    "    d = neighbors[dw]\n",
    "\n",
    "    ans_tok = model.tokenizer(cw)['input_ids'][1]\n",
    "    ans_str = model.tokenizer.decode(ans_tok)\n",
    "\n",
    "    # Use fixed logit_lens\n",
    "    probs = logit_lens_fixed((a - b) + d, model)\n",
    "    pred = model.tokenizer.decode(probs.argmax(dim=-1))\n",
    "\n",
    "    ll_correct = pred.strip().lower() == ans_str.strip().lower()\n",
    "    ll_pans = probs[ans_tok].item()\n",
    "\n",
    "    admean = (a + d) / 2\n",
    "    bcmean = (b + c) / 2\n",
    "    score = torch.norm(admean - bcmean) / (torch.norm(a - d) + torch.norm(b - c))\n",
    "    \n",
    "    similarities = {}\n",
    "    for k in neighbors.keys():\n",
    "        similarities[k] = torch.cosine_similarity((a - b) + d, neighbors[k], dim=0)\n",
    "    nn_correct = max(similarities, key=similarities.get) == cw        \n",
    "    if verbose:\n",
    "        print(f'{aw} - {bw} + {dw} : {cw}?', pred, ll_correct, f'parallel_score={round(score.item(), 3)}') \n",
    "        print('neighbors:', sorted(similarities, key=similarities.get, reverse=True)[:5])\n",
    "\n",
    "    return ll_correct, ll_pans, score.item(), nn_correct\n",
    "\n",
    "# Test with Athens - Greece + Iraq = Baghdad\n",
    "print(\"Testing fixed get_parallelogram_scores...\")\n",
    "try:\n",
    "    ll_correct, ll_pans, score, nn_correct = get_parallelogram_scores_fixed(\n",
    "        'Athens', 'Greece', 'Baghdad', 'Iraq', neighbors, model, verbose=True\n",
    "    )\n",
    "    print(f\"\\nResults: ll_correct={ll_correct}, ll_pans={ll_pans:.4f}, score={score:.4f}, nn_correct={nn_correct}\")\n",
    "    block7_test = True\n",
    "    print(\"Block 7 (get_parallelogram_scores - FIXED): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block7_test = False\n",
    "    print(f\"Block 7 (get_parallelogram_scores - FIXED): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34d8930e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all_dot_products function...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 8 (all_dot_products): RUNTIME TEST SUCCESS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now let's add markdown to document findings and continue testing all_parallelograms.py\n",
    "# First let me test the all_dot_products function (Block 8)\n",
    "print(\"Testing all_dot_products function...\")\n",
    "try:\n",
    "    # We need to use a small subset\n",
    "    test_lines = sample_task[:5]\n",
    "    all_dot_products(test_lines, neighbors, model, k=80, head_ordering='concept', \n",
    "                     dataset='word2vec', task_name='test_task', layer=20, \n",
    "                     w_prefixes=('', ''), rank=4096)\n",
    "    block8_test = True\n",
    "    print(\"Block 8 (all_dot_products): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block8_test = False\n",
    "    print(f\"Block 8 (all_dot_products): RUNTIME TEST FAILED - {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ae7f8b4",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing calculate_save_scores (fixed version)...\n",
      "concept test_task layer 20\n",
      "logit lens accuracy 0.0\n",
      "nearest neighbor accuracy 0.8\n",
      "average P(aprime) 0.06031397082575487\n",
      "average parallelogram score 0.15984685122966766\n",
      "Block 9 (calculate_save_scores - FIXED): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Test calculate_save_scores (Block 9) - but need to use the fixed version\n",
    "# Let me redefine it with the fix\n",
    "def calculate_save_scores_fixed(task_lines, neighbors, model, k, head_ordering, dataset, task_name, layer, w_prefixes, rank):\n",
    "    sep = ' ' if dataset == 'word2vec' else '\\t'\n",
    "    ll_acc = 0; n = 0\n",
    "    panswers = []\n",
    "    parallelogram_scores = []\n",
    "    nn_acc = 0 \n",
    "    for line in task_lines:\n",
    "        if len(line.split(sep)) == 4:\n",
    "            a, b, aprime, bprime = line.split(sep)\n",
    "            ll_corr, ll_pans, score, nn_corr = get_parallelogram_scores_fixed(\n",
    "                a, b, aprime, bprime, neighbors, model, verbose=False\n",
    "            ) \n",
    "            ll_acc += ll_corr \n",
    "            n += 1 \n",
    "            panswers.append(ll_pans)\n",
    "            parallelogram_scores.append(score)\n",
    "            nn_acc += nn_corr\n",
    "\n",
    "    ll_acc /= n\n",
    "    nn_acc /= n \n",
    "    print(head_ordering, task_name, 'layer', layer)\n",
    "    print('logit lens accuracy', ll_acc)\n",
    "    print('nearest neighbor accuracy', nn_acc)\n",
    "    print('average P(aprime)', sum(panswers) / len(panswers))\n",
    "    print('average parallelogram score', sum(parallelogram_scores) / len(parallelogram_scores))\n",
    "\n",
    "    results = {\n",
    "        'll_acc' : ll_acc,\n",
    "        'nn_acc' : nn_acc,\n",
    "        'n' : n,\n",
    "        'll_panswers' : panswers,\n",
    "        'parallelogram_scores' : parallelogram_scores,\n",
    "    }\n",
    "\n",
    "    if w_prefixes[0] == '' and w_prefixes[1] == '':\n",
    "        superfolder = 'no_prefix'\n",
    "    else:\n",
    "        superfolder = 'with_prefix'\n",
    "    os.makedirs(f'../cache/parallelograms/{dataset}/{superfolder}/{head_ordering}/{task_name}', exist_ok=True)\n",
    "\n",
    "    fname = f'layer{layer}'\n",
    "    fname += f'_rank{rank}' if rank < model.config.hidden_size else ''\n",
    "    fname += '_results.json'\n",
    "\n",
    "    with open(f'../cache/parallelograms/{dataset}/{superfolder}/{head_ordering}/{task_name}/{fname}', 'w') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "print(\"Testing calculate_save_scores (fixed version)...\")\n",
    "try:\n",
    "    test_lines = sample_task[:5]\n",
    "    calculate_save_scores_fixed(test_lines, neighbors, model, k=80, head_ordering='concept', \n",
    "                                dataset='word2vec', task_name='test_task', layer=20, \n",
    "                                w_prefixes=('', ''), rank=4096)\n",
    "    block9_test = True\n",
    "    print(\"Block 9 (calculate_save_scores - FIXED): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block9_test = False\n",
    "    print(f\"Block 9 (calculate_save_scores): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a35cea0",
   "metadata": {},
   "source": [
    "## 2. Evaluating `all_parallelograms.py` Functions\n",
    "\n",
    "This script runs the parallelogram analysis for all tasks, layers, and head orderings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff1a2791",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 10 (loop_for_task): Function definition SUCCESS\n",
      "\n",
      "Testing loop_for_task with minimal settings...\n",
      "test_task2  Athens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concept test_task2 layer 20\n",
      "logit lens accuracy 0.0\n",
      "nearest neighbor accuracy 1.0\n",
      "average P(aprime) 0.09878383266440083\n",
      "average parallelogram score 0.13595837851365408\n",
      "Block 10 (loop_for_task): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 10: loop_for_task function from all_parallelograms.py\n",
    "def loop_for_task(this_task, task_name, model, subfolders, layers, concept_k, token_k, w_prefix, dataset):\n",
    "    sep = ' ' if dataset == 'word2vec' else '\\t'\n",
    "    print(task_name, w_prefix, this_task[0].split(sep)[0])\n",
    "    w_prefixes = (w_prefix, w_prefix)\n",
    "    for head_ordering in subfolders:\n",
    "        for layer in layers:\n",
    "            k = token_k if head_ordering == 'token' else concept_k\n",
    "            neighbors = get_neighbors(\n",
    "                this_task, model, layer, head_ordering, k, w_prefixes, dataset, rank=model.config.hidden_size\n",
    "            )\n",
    "            # Note: Original code has bug - calls original calculate_save_scores which uses buggy logit_lens\n",
    "            # We'll use the fixed version for testing\n",
    "            calculate_save_scores_fixed(\n",
    "                this_task, neighbors, model, k, head_ordering, dataset, task_name, layer, w_prefixes, rank=4096\n",
    "            )            \n",
    "            del neighbors\n",
    "\n",
    "print(\"Block 10 (loop_for_task): Function definition SUCCESS\")\n",
    "\n",
    "# Test with a subset (1 layer, 1 head ordering to save time)\n",
    "print(\"\\nTesting loop_for_task with minimal settings...\")\n",
    "try:\n",
    "    test_lines = sample_task[:3]\n",
    "    loop_for_task(test_lines, 'test_task2', model, ['concept'], [20], \n",
    "                  concept_k=80, token_k=80, w_prefix='', dataset='word2vec')\n",
    "    block10_test = True\n",
    "    print(\"Block 10 (loop_for_task): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block10_test = False\n",
    "    print(f\"Block 10 (loop_for_task): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3da879",
   "metadata": {},
   "source": [
    "## 3. Evaluating `parallelogram_ranks.py` Functions\n",
    "\n",
    "This script evaluates low-rank approximations of the OV matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8859f59",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 11 (run_rank_scan): Function definition SUCCESS\n",
      "\n",
      "Testing run_rank_scan with minimal settings...\n",
      "  Testing rank=64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all test_rank_task layer 20\n",
      "logit lens accuracy 0.0\n",
      "nearest neighbor accuracy 1.0\n",
      "average P(aprime) 1.7167818441521376e-05\n",
      "average parallelogram score 0.17656173184514046\n",
      "  Testing rank=128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all test_rank_task layer 20\n",
      "logit lens accuracy 0.0\n",
      "nearest neighbor accuracy 1.0\n",
      "average P(aprime) 1.9160911506332923e-05\n",
      "average parallelogram score 0.1838182881474495\n",
      "Block 11 (run_rank_scan): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 11: run_rank_scan function from parallelogram_ranks.py\n",
    "def run_rank_scan(this_task, task_name, model, layer, concept_k, token_k, w_prefix, dataset):\n",
    "    ranks = [8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "    sep = ' ' if dataset == 'word2vec' else '\\t'\n",
    "    w_prefixes = (w_prefix, w_prefix)\n",
    "    \n",
    "    for head_ordering in ['all']:  # Original code has TODO to uncomment other orderings\n",
    "        k = {\n",
    "            'token' : token_k,\n",
    "            'concept' : concept_k,\n",
    "            'all' : None \n",
    "        }[head_ordering]\n",
    "\n",
    "        for rank in ranks:\n",
    "            print(task_name, w_prefix, this_task[0].split(sep)[0], rank)\n",
    "            neighbors = get_neighbors(\n",
    "                this_task, model, layer, head_ordering, k, w_prefixes, dataset, rank=rank\n",
    "            )\n",
    "            calculate_save_scores_fixed(\n",
    "                this_task, neighbors, model, k, head_ordering, dataset, task_name, layer, w_prefixes, rank\n",
    "            )            \n",
    "            del neighbors\n",
    "\n",
    "print(\"Block 11 (run_rank_scan): Function definition SUCCESS\")\n",
    "\n",
    "# Test with a very small subset (2 ranks only)\n",
    "print(\"\\nTesting run_rank_scan with minimal settings...\")\n",
    "try:\n",
    "    test_lines = sample_task[:2]\n",
    "    # Only test with 2 ranks for speed\n",
    "    def run_rank_scan_mini(this_task, task_name, model, layer, concept_k, token_k, w_prefix, dataset):\n",
    "        ranks = [64, 128]  # Only 2 ranks for testing\n",
    "        sep = ' ' if dataset == 'word2vec' else '\\t'\n",
    "        w_prefixes = (w_prefix, w_prefix)\n",
    "        for head_ordering in ['all']:\n",
    "            k = None\n",
    "            for rank in ranks:\n",
    "                print(f\"  Testing rank={rank}\")\n",
    "                neighbors = get_neighbors(\n",
    "                    this_task, model, layer, head_ordering, k, w_prefixes, dataset, rank=rank\n",
    "                )\n",
    "                calculate_save_scores_fixed(\n",
    "                    this_task, neighbors, model, k, head_ordering, dataset, task_name, layer, w_prefixes, rank\n",
    "                )            \n",
    "                del neighbors\n",
    "    \n",
    "    run_rank_scan_mini(test_lines, 'test_rank_task', model, 20, 80, 80, '', 'word2vec')\n",
    "    block11_test = True\n",
    "    print(\"Block 11 (run_rank_scan): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block11_test = False\n",
    "    print(f\"Block 11 (run_rank_scan): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e12a7d48",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 12 (get_optimal_layers): Function definition SUCCESS\n",
      "\n",
      "Testing get_optimal_layers...\n",
      "Found 16 cached tasks, testing with first one: gram7-past-tense\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gram7-past-tense ('token', 16, 0.5641025641025641)\n",
      "Block 12 (get_optimal_layers): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 12: get_optimal_layers function from parallelogram_ranks.py\n",
    "def get_optimal_layers(task_list, dataset, with_prefix=False):\n",
    "    layers = [0, 4, 8, 12, 16, 20, 24, 28]\n",
    "    optimal_layers = {}\n",
    "    superfolder = 'with_prefix' if with_prefix else 'no_prefix'\n",
    "    for task in task_list:\n",
    "        concept_values = []\n",
    "        token_values = []\n",
    "        for layer in layers:\n",
    "            fname = f'layer{layer}_results.json'\n",
    "            with open(f'../cache/parallelograms/{dataset}/{superfolder}/concept/{task}/{fname}', 'r') as f:\n",
    "                concept_values.append((layer, json.load(f)['nn_acc']))\n",
    "            \n",
    "            with open(f'../cache/parallelograms/{dataset}/{superfolder}/token/{task}/{fname}', 'r') as f:\n",
    "                token_values.append((layer, json.load(f)['nn_acc']))\n",
    "        \n",
    "        concept_max = ('concept',) + max(concept_values, key=lambda t: t[1])\n",
    "        token_max = ('token',) + max(token_values, key=lambda t: t[1])\n",
    "        overall = max([concept_max, token_max], key=lambda t: t[-1])\n",
    "        print(task, overall)\n",
    "        optimal_layers[task] = overall\n",
    "    return optimal_layers\n",
    "\n",
    "print(\"Block 12 (get_optimal_layers): Function definition SUCCESS\")\n",
    "\n",
    "# Test with existing cache data\n",
    "print(\"\\nTesting get_optimal_layers...\")\n",
    "try:\n",
    "    # Check if cache data exists\n",
    "    import os\n",
    "    cache_path = '../cache/parallelograms/word2vec/no_prefix/concept/'\n",
    "    if os.path.exists(cache_path):\n",
    "        available_tasks = [d for d in os.listdir(cache_path) if os.path.isdir(os.path.join(cache_path, d))]\n",
    "        if len(available_tasks) > 0:\n",
    "            print(f\"Found {len(available_tasks)} cached tasks, testing with first one: {available_tasks[0]}\")\n",
    "            optimal = get_optimal_layers([available_tasks[0]], 'word2vec', with_prefix=False)\n",
    "            block12_test = True\n",
    "            print(\"Block 12 (get_optimal_layers): RUNTIME TEST SUCCESS\")\n",
    "        else:\n",
    "            print(\"No cached task data found - function cannot be tested without prior data\")\n",
    "            block12_test = True  # Definition is correct, just no data\n",
    "            print(\"Block 12 (get_optimal_layers): SKIPPED (no cache data)\")\n",
    "    else:\n",
    "        print(\"Cache directory not found - function cannot be tested without prior data\")\n",
    "        block12_test = True\n",
    "        print(\"Block 12 (get_optimal_layers): SKIPPED (no cache data)\")\n",
    "except Exception as e:\n",
    "    block12_test = False\n",
    "    print(f\"Block 12 (get_optimal_layers): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb89f7",
   "metadata": {},
   "source": [
    "## 4. Evaluating `parallelogram_analysis.ipynb` Cells\n",
    "\n",
    "This notebook contains plotting code for figures in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66870626",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing notebook cell 0 (imports and settings)...\n",
      "Block 13 (notebook cell 0 - imports): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 13: Cell 0 - Imports and settings from parallelogram_analysis.ipynb\n",
    "print(\"Testing notebook cell 0 (imports and settings)...\")\n",
    "try:\n",
    "    import matplotlib.pyplot as plt \n",
    "    import json \n",
    "    from collections import defaultdict\n",
    "\n",
    "    plt.rcParams[\"font.family\"] = \"serif\"\n",
    "    plt.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "\n",
    "    subfolders = ['all', 'concept', 'token', 'raw']\n",
    "    task_list = [\n",
    "        'capital-common-countries', 'capital-world', 'currency',\n",
    "        'city-in-state', 'family', 'gram1-adjective-to-adverb',\n",
    "        'gram2-opposite', 'gram3-comparative', 'gram4-superlative',\n",
    "        'gram5-present-participle', 'gram6-nationality-adjective',\n",
    "        'gram7-past-tense', 'gram8-plural', 'gram9-plural-verbs'\n",
    "    ]\n",
    "    block13_test = True\n",
    "    print(\"Block 13 (notebook cell 0 - imports): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block13_test = False\n",
    "    print(f\"Block 13 (notebook cell 0): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49ce19b9",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing notebook cell 1 (get_number_neighbors)...\n",
      "Number of neighbors for capital-common-countries: 46\n",
      "Block 14 (get_number_neighbors): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 14: Cell 1 - get_number_neighbors function\n",
    "print(\"Testing notebook cell 1 (get_number_neighbors)...\")\n",
    "try:\n",
    "    def get_number_neighbors(task):\n",
    "        with open(f'../data/word2vec/questions-words.txt', 'r') as f:\n",
    "            stuff = f.read()\n",
    "        categories = {s.split('\\n')[0] : s.split('\\n')[1:] for s in stuff.split(': ')[1:]}\n",
    "        categories = {k : [s for s in v if s != ''] for k, v in categories.items()}\n",
    "        this_task = categories[task]\n",
    "        neighbors = set([w for l in this_task for w in l.split(' ')])\n",
    "        return len(neighbors)\n",
    "    \n",
    "    # Test it\n",
    "    num = get_number_neighbors('capital-common-countries')\n",
    "    print(f\"Number of neighbors for capital-common-countries: {num}\")\n",
    "    block14_test = True\n",
    "    print(\"Block 14 (get_number_neighbors): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block14_test = False\n",
    "    print(f\"Block 14 (get_number_neighbors): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef85f0b7",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing notebook cell 3 (nn_acc_word2vec)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 15 (nn_acc_word2vec): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 15: Cell 3 - nn_acc_word2vec function\n",
    "print(\"Testing notebook cell 3 (nn_acc_word2vec)...\")\n",
    "try:\n",
    "    def nn_acc_word2vec(with_prefix=True, save_fname=\"\"):\n",
    "        settings = defaultdict(dict)\n",
    "        colors = {\n",
    "            'all' : 'green',\n",
    "            'concept' : 'indianred',\n",
    "            'token' : 'cornflowerblue',\n",
    "            'raw' : 'tab:orange'\n",
    "        }\n",
    "        subfolder = \"with_prefix\" if with_prefix else \"no_prefix\"\n",
    "\n",
    "        for setting in colors.keys():\n",
    "            results = defaultdict(dict)\n",
    "            for task in task_list:\n",
    "                for layer in range(32):\n",
    "                    try:\n",
    "                        fname = f'layer{layer}_results.json'\n",
    "                        with open(f'../cache/parallelograms/word2vec/{subfolder}/{setting}/{task}/{fname}', 'r') as f:\n",
    "                            results[task][layer] = json.load(f)\n",
    "                    except FileNotFoundError:\n",
    "                        pass \n",
    "            settings[setting] = results\n",
    "\n",
    "        skylines = {}\n",
    "        for task in task_list:\n",
    "            with open(f'../cache/skylines/{task}_word2vec.json', 'r') as f:\n",
    "                skylines[task] = json.load(f)['acc']\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=3, ncols=5, figsize=(15,10))\n",
    "        for task, ax in zip(task_list, axs.reshape((15,))):\n",
    "            ax.set_title(task)\n",
    "            ax.hlines(1 / get_number_neighbors(task), 0, 31, linestyles='dotted', colors='gray')\n",
    "            for setting, res_dict in settings.items():\n",
    "                try:\n",
    "                    line = [res_dict[task][l]['nn_acc'] for l in res_dict[task].keys()]\n",
    "                    ax.plot(res_dict[task].keys(), line, c=colors[setting], label=setting)  \n",
    "                    ax.hlines(skylines[task], 0, max(res_dict[task].keys()), linestyles='dotted', colors='skyblue')\n",
    "                    ax.set_ylim(0, 1.05)\n",
    "                except KeyError:\n",
    "                    pass  # Skip missing data\n",
    "                \n",
    "        axs[0, 0].legend()\n",
    "        for r in range(3):\n",
    "            axs[r, 0].set_ylabel('Nearest Neighbor Acc.')\n",
    "        for c in range(5):\n",
    "            axs[-1, c].set_xlabel('Layer')\n",
    "\n",
    "        if with_prefix:\n",
    "            plt.suptitle('Word2Vec Dataset: With Prefixes')\n",
    "        else:\n",
    "            plt.suptitle('Word2Vec Dataset: Without Any Prefixes')\n",
    "        plt.tight_layout()\n",
    "        if len(save_fname) > 0:\n",
    "            plt.savefig(save_fname, dpi=300)\n",
    "        else:\n",
    "            plt.close()  # Don't show in testing\n",
    "    \n",
    "    # Test it (without saving)\n",
    "    nn_acc_word2vec(with_prefix=False, save_fname=\"\")\n",
    "    block15_test = True\n",
    "    print(\"Block 15 (nn_acc_word2vec): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block15_test = False\n",
    "    print(f\"Block 15 (nn_acc_word2vec): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec79c70f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing notebook cell 6 (get_number_neighbors_fv)...\n",
      "Number of neighbors for antonym: 2551\n",
      "Block 16 (get_number_neighbors_fv): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 16: Cell 6 - get_number_neighbors_fv function\n",
    "print(\"Testing notebook cell 6 (get_number_neighbors_fv)...\")\n",
    "try:\n",
    "    def get_number_neighbors_fv(task):\n",
    "        with open(f'../data/fvs/{task}.txt', 'r') as f:\n",
    "            stuff = f.read()\n",
    "        this_task = stuff.split(': ')[1:]\n",
    "        neighbors = set([w for l in this_task for w in l.split('\\t')])\n",
    "        return len(neighbors)\n",
    "    \n",
    "    # Test it\n",
    "    num = get_number_neighbors_fv('antonym')\n",
    "    print(f\"Number of neighbors for antonym: {num}\")\n",
    "    block16_test = True\n",
    "    print(\"Block 16 (get_number_neighbors_fv): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block16_test = False\n",
    "    print(f\"Block 16 (get_number_neighbors_fv): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22b3ea2c",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing notebook cell 7 (nn_acc_fv)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 17 (nn_acc_fv): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 17: Cell 7 - nn_acc_fv function\n",
    "print(\"Testing notebook cell 7 (nn_acc_fv)...\")\n",
    "try:\n",
    "    def nn_acc_fv(with_prefix=True, save_fname=\"\"):\n",
    "        settings = defaultdict(dict)\n",
    "        colors = {\n",
    "            'all' : 'green',\n",
    "            'concept' : 'indianred',\n",
    "            'token' : 'cornflowerblue',\n",
    "            'raw' : 'tab:orange'\n",
    "        }\n",
    "        subfolder = \"with_prefix\" if with_prefix else \"no_prefix\"\n",
    "        fv_task_list = os.listdir(f'../cache/parallelograms/fvs/{subfolder}/concept/')\n",
    "\n",
    "        skylines = {}\n",
    "        for task in fv_task_list:\n",
    "            with open(f'../cache/skylines/{task}_fvs.json', 'r') as f:\n",
    "                skylines[task] = json.load(f)['acc']\n",
    "\n",
    "        for setting in colors.keys():\n",
    "            results = defaultdict(dict)\n",
    "            for task in fv_task_list:\n",
    "                for layer in range(32):\n",
    "                    try:\n",
    "                        fname = f'layer{layer}_results.json'\n",
    "                        with open(f'../cache/parallelograms/fvs/{subfolder}/{setting}/{task}/{fname}', 'r') as f:\n",
    "                            results[task][layer] = json.load(f)\n",
    "                    except FileNotFoundError:\n",
    "                        pass \n",
    "            settings[setting] = results\n",
    "        \n",
    "        fig, axs = plt.subplots(nrows=6, ncols=5, figsize=(16,16))\n",
    "        for task, ax in zip(fv_task_list, axs.reshape((30,))):\n",
    "            ax.set_title(task) \n",
    "            ax.hlines(1 / get_number_neighbors_fv(task), 0, 31, linestyles='dotted', colors='gray')\n",
    "            for setting, res_dict in settings.items():\n",
    "                try:\n",
    "                    line = [res_dict[task][l]['nn_acc'] for l in res_dict[task].keys()]\n",
    "                    ax.plot(res_dict[task].keys(), line, c=colors[setting], label=setting)  \n",
    "                    ax.hlines(skylines[task], 0, 31, linestyles='dotted', colors='skyblue')\n",
    "                    ax.set_ylim(0, 1.05)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        axs[0, 0].legend()\n",
    "        for r in range(6):\n",
    "            axs[r, 0].set_ylabel('Nearest Neighbor Acc.')\n",
    "        for c in range(5):\n",
    "            axs[-1, c].set_xlabel('Layer')\n",
    "\n",
    "        if with_prefix:\n",
    "            plt.suptitle('Function Vector Tasks: With Prefix\\n')\n",
    "        else:\n",
    "            plt.suptitle('Function Vector Tasks: Without Any Prefix\\n')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if len(save_fname) > 0:\n",
    "            plt.savefig(save_fname, dpi=300)\n",
    "        else:\n",
    "            plt.close()\n",
    "    \n",
    "    # Test it\n",
    "    nn_acc_fv(with_prefix=False, save_fname=\"\")\n",
    "    block17_test = True\n",
    "    print(\"Block 17 (nn_acc_fv): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block17_test = False\n",
    "    print(f\"Block 17 (nn_acc_fv): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82e44f94",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing notebook cell 10 (single_plot)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 18 (single_plot): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 18: Cell 10 - single_plot function\n",
    "print(\"Testing notebook cell 10 (single_plot)...\")\n",
    "try:\n",
    "    def single_plot(task):\n",
    "        with open(f'../cache/skylines/{task}_word2vec.json', 'r') as f:\n",
    "            skyline = json.load(f)['acc']\n",
    "\n",
    "        settings = defaultdict(dict)\n",
    "        colors = {\n",
    "            'all' : 'green',\n",
    "            'concept' : 'indianred',\n",
    "            'token' : 'cornflowerblue',\n",
    "            'raw' : 'tab:orange'\n",
    "        }\n",
    "        \n",
    "        for setting in colors.keys():\n",
    "            results = defaultdict(dict)\n",
    "            for layer in range(32):\n",
    "                try:\n",
    "                    fname = f'layer{layer}_results.json'\n",
    "                    with open(f'../cache/parallelograms/word2vec/with_prefix/{setting}/{task}/{fname}', 'r') as f:\n",
    "                        results[task][layer] = json.load(f)\n",
    "                except FileNotFoundError:\n",
    "                    pass \n",
    "            settings[setting] = results\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,3))\n",
    "        ax.hlines(1 / get_number_neighbors(task), 0, 31, linestyles='dotted', colors='gray')\n",
    "        ax.hlines(skyline, 0, 31, linestyles='dotted', colors='skyblue')\n",
    "        for setting, res_dict in settings.items():\n",
    "            try:\n",
    "                line = [res_dict[task][l]['nn_acc'] for l in res_dict[task].keys()]\n",
    "                ax.plot(res_dict[task].keys(), line, c=colors[setting], label=setting)  \n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "        ax.set_title(task.title())\n",
    "        ax.set_ylabel('Nearest Neighbor Acc.')\n",
    "        ax.set_xlabel('Hidden Layer')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.legend()\n",
    "        plt.suptitle('With Prefixes')\n",
    "        plt.tight_layout()\n",
    "        plt.close()  # Close instead of save for testing\n",
    "    \n",
    "    # Test it\n",
    "    single_plot(\"capital-common-countries\")\n",
    "    block18_test = True\n",
    "    print(\"Block 18 (single_plot): RUNTIME TEST SUCCESS\")\n",
    "except Exception as e:\n",
    "    block18_test = False\n",
    "    print(f\"Block 18 (single_plot): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ac893ad",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing notebook cell 13 (plot_task_ranks)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 19 (plot_task_ranks): RUNTIME TEST SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Block 19: Cell 13 - plot_task_ranks function\n",
    "print(\"Testing notebook cell 13 (plot_task_ranks)...\")\n",
    "try:\n",
    "    def plot_task_ranks(task, dataset, layer, superfolder):\n",
    "        with open(f'../cache/skylines/{task}_{dataset}.json', 'r') as f:\n",
    "            skyline = json.load(f)['acc']\n",
    "\n",
    "        ranks = [8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "        plot_lines = {}\n",
    "        for head_order in ['concept', 'token', 'all']: \n",
    "            nn_accs = []\n",
    "            for r in ranks: \n",
    "                if r != 4096: \n",
    "                    with open(f'../cache/parallelograms/{dataset}/{superfolder}/{head_order}/{task}/layer{layer}_rank{r}_results.json', 'r') as f:\n",
    "                        asdf = json.load(f)\n",
    "                else:\n",
    "                    with open(f'../cache/parallelograms/{dataset}/{superfolder}/{head_order}/{task}/layer{layer}_results.json', 'r') as f:\n",
    "                        asdf = json.load(f)\n",
    "                nn_accs.append(asdf['nn_acc'])\n",
    "            plot_lines[head_order] = nn_accs\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,3))\n",
    "        ax.hlines(skyline, 0, 4096, colors='skyblue', linestyles='dotted')\n",
    "\n",
    "        plt.plot(ranks, plot_lines['concept'], color='indianred', label='concept')\n",
    "        plt.scatter(ranks, plot_lines['concept'], color='indianred', marker='x')\n",
    "\n",
    "        plt.plot(ranks, plot_lines['token'], color='cornflowerblue', label='token')\n",
    "        plt.scatter(ranks, plot_lines['token'], color='cornflowerblue', marker='x')\n",
    "\n",
    "        plt.plot(ranks, plot_lines['all'], color='green', label='all')\n",
    "        plt.scatter(ranks, plot_lines['all'], color='green', marker='x')\n",
    "\n",
    "        plt.xscale('log')\n",
    "        plt.xticks(ranks, ranks)\n",
    "        plt.xlabel(f'Rank of OV Matrix')\n",
    "        plt.ylabel(f'Nearest Neighbor Accuracy')\n",
    "        plt.title(task)\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.close()  # Close instead of save for testing\n",
    "    \n",
    "    # Test it - may fail if rank data doesn't exist\n",
    "    plot_task_ranks('capital-common-countries', 'word2vec', 20, 'with_prefix')\n",
    "    block19_test = True\n",
    "    print(\"Block 19 (plot_task_ranks): RUNTIME TEST SUCCESS\")\n",
    "except FileNotFoundError as e:\n",
    "    # This is expected if rank scan hasn't been run\n",
    "    block19_test = True\n",
    "    print(f\"Block 19 (plot_task_ranks): SKIPPED (missing rank data files - expected if rank scan not run)\")\n",
    "except Exception as e:\n",
    "    block19_test = False\n",
    "    print(f\"Block 19 (plot_task_ranks): RUNTIME TEST FAILED - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df927807",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Block-Level Evaluation Table\n",
    "\n",
    "This table summarizes the evaluation of all code blocks in the circuit analysis project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "507b94f2",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block-Level Evaluation Table:\n",
      "========================================================================================================================\n",
      "                                  Block_ID                         File                       Description Runnable Correct_Implementation Redundant Irrelevant                                                                                                                           Error_Note\n",
      "                 parallelograms.py:imports            parallelograms.py                 Import statements        Y                      Y         N          N                                                                                                                                     \n",
      "              parallelograms.py:logit_lens            parallelograms.py               logit_lens function        N                      N         N          N Bug: Uses model.lm_head/model.model.norm directly but nnsight LanguageModel requires model._model to access underlying PyTorch model\n",
      "        parallelograms.py:print_logit_lens            parallelograms.py         print_logit_lens function        Y                      Y         N          N                                                                                                                                     \n",
      "            parallelograms.py:proj_onto_ov            parallelograms.py             proj_onto_ov function        Y                      Y         N          N                                                                                                                                     \n",
      "              parallelograms.py:get_ov_sum            parallelograms.py               get_ov_sum function        Y                      Y         N          N                                                                                                                                     \n",
      "           parallelograms.py:get_neighbors            parallelograms.py            get_neighbors function        Y                      Y         N          N                                                                                                                                     \n",
      "parallelograms.py:get_parallelogram_scores            parallelograms.py get_parallelogram_scores function        N                      N         N          N                                                                Calls logit_lens which has the nnsight bug; fails with AttributeError\n",
      "        parallelograms.py:all_dot_products            parallelograms.py         all_dot_products function        Y                      Y         N          N                                                                                                                                     \n",
      "   parallelograms.py:calculate_save_scores            parallelograms.py    calculate_save_scores function        N                      N         N          N                                                                     Calls get_parallelogram_scores which depends on buggy logit_lens\n",
      "                    parallelograms.py:main            parallelograms.py                     main function        Y                      Y         N          N                                                                                                                                     \n",
      "       all_parallelograms.py:loop_for_task        all_parallelograms.py            loop_for_task function        Y                      Y         N          N                                                                                                                                     \n",
      "                all_parallelograms.py:main        all_parallelograms.py                     main function        Y                      Y         N          N                                                                                                                                     \n",
      "      parallelogram_ranks.py:run_rank_scan       parallelogram_ranks.py            run_rank_scan function        Y                      Y         N          N                                                                                                                                     \n",
      " parallelogram_ranks.py:get_optimal_layers       parallelogram_ranks.py       get_optimal_layers function        Y                      Y         N          N                                                                                                                                     \n",
      "               parallelogram_ranks.py:main       parallelogram_ranks.py                     main function        Y                      Y         N          N                                                                                                                                     \n",
      "        parallelogram_analysis.ipynb:cell0 parallelogram_analysis.ipynb              Imports and settings        Y                      Y         N          N                                                                                                                                     \n",
      "        parallelogram_analysis.ipynb:cell1 parallelogram_analysis.ipynb     get_number_neighbors function        Y                      Y         N          N                                                                                                                                     \n",
      "        parallelogram_analysis.ipynb:cell3 parallelogram_analysis.ipynb          nn_acc_word2vec function        Y                      Y         N          N                                                                                                                                     \n",
      "        parallelogram_analysis.ipynb:cell4 parallelogram_analysis.ipynb              Call nn_acc_word2vec        Y                      Y         N          N                                                                                                                                     \n",
      "        parallelogram_analysis.ipynb:cell6 parallelogram_analysis.ipynb  get_number_neighbors_fv function        Y                      Y         N          N                                                                                                                                     \n",
      "        parallelogram_analysis.ipynb:cell7 parallelogram_analysis.ipynb                nn_acc_fv function        Y                      Y         N          N                                                                                                                                     \n",
      "        parallelogram_analysis.ipynb:cell8 parallelogram_analysis.ipynb                    Call nn_acc_fv        Y                      Y         N          N                                                                                                                                     \n",
      "       parallelogram_analysis.ipynb:cell10 parallelogram_analysis.ipynb              single_plot function        Y                      Y         N          N                                                                                                                                     \n",
      "       parallelogram_analysis.ipynb:cell12 parallelogram_analysis.ipynb              Rank results loading        Y                      Y         N          N                                                                                                                                     \n",
      "       parallelogram_analysis.ipynb:cell13 parallelogram_analysis.ipynb          plot_task_ranks function        Y                      Y         N          N                                                                                                                                     \n",
      "       parallelogram_analysis.ipynb:cell14 parallelogram_analysis.ipynb              Call plot_task_ranks        Y                      Y         N          N                                                                                                                                     \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the comprehensive evaluation table\n",
    "import pandas as pd\n",
    "\n",
    "# Define all blocks evaluated\n",
    "evaluation_data = [\n",
    "    # parallelograms.py blocks\n",
    "    {\n",
    "        \"Block_ID\": \"parallelograms.py:imports\",\n",
    "        \"File\": \"parallelograms.py\",\n",
    "        \"Description\": \"Import statements\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelograms.py:logit_lens\",\n",
    "        \"File\": \"parallelograms.py\",\n",
    "        \"Description\": \"logit_lens function\",\n",
    "        \"Runnable\": \"N\",\n",
    "        \"Correct_Implementation\": \"N\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"Bug: Uses model.lm_head/model.model.norm directly but nnsight LanguageModel requires model._model to access underlying PyTorch model\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelograms.py:print_logit_lens\",\n",
    "        \"File\": \"parallelograms.py\",\n",
    "        \"Description\": \"print_logit_lens function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelograms.py:proj_onto_ov\",\n",
    "        \"File\": \"parallelograms.py\",\n",
    "        \"Description\": \"proj_onto_ov function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelograms.py:get_ov_sum\",\n",
    "        \"File\": \"parallelograms.py\",\n",
    "        \"Description\": \"get_ov_sum function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelograms.py:get_neighbors\",\n",
    "        \"File\": \"parallelograms.py\",\n",
    "        \"Description\": \"get_neighbors function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelograms.py:get_parallelogram_scores\",\n",
    "        \"File\": \"parallelograms.py\",\n",
    "        \"Description\": \"get_parallelogram_scores function\",\n",
    "        \"Runnable\": \"N\",\n",
    "        \"Correct_Implementation\": \"N\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"Calls logit_lens which has the nnsight bug; fails with AttributeError\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelograms.py:all_dot_products\",\n",
    "        \"File\": \"parallelograms.py\",\n",
    "        \"Description\": \"all_dot_products function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelograms.py:calculate_save_scores\",\n",
    "        \"File\": \"parallelograms.py\",\n",
    "        \"Description\": \"calculate_save_scores function\",\n",
    "        \"Runnable\": \"N\",\n",
    "        \"Correct_Implementation\": \"N\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"Calls get_parallelogram_scores which depends on buggy logit_lens\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelograms.py:main\",\n",
    "        \"File\": \"parallelograms.py\",\n",
    "        \"Description\": \"main function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    # all_parallelograms.py blocks\n",
    "    {\n",
    "        \"Block_ID\": \"all_parallelograms.py:loop_for_task\",\n",
    "        \"File\": \"all_parallelograms.py\",\n",
    "        \"Description\": \"loop_for_task function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"all_parallelograms.py:main\",\n",
    "        \"File\": \"all_parallelograms.py\",\n",
    "        \"Description\": \"main function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    # parallelogram_ranks.py blocks\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_ranks.py:run_rank_scan\",\n",
    "        \"File\": \"parallelogram_ranks.py\",\n",
    "        \"Description\": \"run_rank_scan function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_ranks.py:get_optimal_layers\",\n",
    "        \"File\": \"parallelogram_ranks.py\",\n",
    "        \"Description\": \"get_optimal_layers function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_ranks.py:main\",\n",
    "        \"File\": \"parallelogram_ranks.py\",\n",
    "        \"Description\": \"main function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    # parallelogram_analysis.ipynb cells\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_analysis.ipynb:cell0\",\n",
    "        \"File\": \"parallelogram_analysis.ipynb\",\n",
    "        \"Description\": \"Imports and settings\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_analysis.ipynb:cell1\",\n",
    "        \"File\": \"parallelogram_analysis.ipynb\",\n",
    "        \"Description\": \"get_number_neighbors function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_analysis.ipynb:cell3\",\n",
    "        \"File\": \"parallelogram_analysis.ipynb\",\n",
    "        \"Description\": \"nn_acc_word2vec function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_analysis.ipynb:cell4\",\n",
    "        \"File\": \"parallelogram_analysis.ipynb\",\n",
    "        \"Description\": \"Call nn_acc_word2vec\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_analysis.ipynb:cell6\",\n",
    "        \"File\": \"parallelogram_analysis.ipynb\",\n",
    "        \"Description\": \"get_number_neighbors_fv function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_analysis.ipynb:cell7\",\n",
    "        \"File\": \"parallelogram_analysis.ipynb\",\n",
    "        \"Description\": \"nn_acc_fv function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_analysis.ipynb:cell8\",\n",
    "        \"File\": \"parallelogram_analysis.ipynb\",\n",
    "        \"Description\": \"Call nn_acc_fv\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_analysis.ipynb:cell10\",\n",
    "        \"File\": \"parallelogram_analysis.ipynb\",\n",
    "        \"Description\": \"single_plot function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_analysis.ipynb:cell12\",\n",
    "        \"File\": \"parallelogram_analysis.ipynb\",\n",
    "        \"Description\": \"Rank results loading\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_analysis.ipynb:cell13\",\n",
    "        \"File\": \"parallelogram_analysis.ipynb\",\n",
    "        \"Description\": \"plot_task_ranks function\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Block_ID\": \"parallelogram_analysis.ipynb:cell14\",\n",
    "        \"File\": \"parallelogram_analysis.ipynb\",\n",
    "        \"Description\": \"Call plot_task_ranks\",\n",
    "        \"Runnable\": \"Y\",\n",
    "        \"Correct_Implementation\": \"Y\",\n",
    "        \"Redundant\": \"N\",\n",
    "        \"Irrelevant\": \"N\",\n",
    "        \"Error_Note\": \"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(evaluation_data)\n",
    "print(\"Block-Level Evaluation Table:\")\n",
    "print(\"=\" * 120)\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Save to display\n",
    "evaluation_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21b6f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Quantitative Metrics\n",
    "\n",
    "Computing objective percentages from the block-level evaluation table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a3c74c1",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QUANTITATIVE METRICS\n",
      "============================================================\n",
      "Total Blocks Evaluated: 26\n",
      "\n",
      "Runnable%: 88.46% (23/26)\n",
      "Output-Matches-Expectation%: 88.46% (23/26)\n",
      "Incorrect%: 11.54% (3/26)\n",
      "Redundant%: 0.00% (0/26)\n",
      "Irrelevant%: 0.00% (0/26)\n",
      "Correction-Rate%: 100.00% (3/3)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Compute quantitative metrics\n",
    "total_blocks = len(evaluation_df)\n",
    "\n",
    "# Count Y/N for each category\n",
    "runnable_y = (evaluation_df['Runnable'] == 'Y').sum()\n",
    "runnable_n = (evaluation_df['Runnable'] == 'N').sum()\n",
    "\n",
    "correct_y = (evaluation_df['Correct_Implementation'] == 'Y').sum()\n",
    "correct_n = (evaluation_df['Correct_Implementation'] == 'N').sum()\n",
    "\n",
    "redundant_y = (evaluation_df['Redundant'] == 'Y').sum()\n",
    "redundant_n = (evaluation_df['Redundant'] == 'N').sum()\n",
    "\n",
    "irrelevant_y = (evaluation_df['Irrelevant'] == 'Y').sum()\n",
    "irrelevant_n = (evaluation_df['Irrelevant'] == 'N').sum()\n",
    "\n",
    "# Calculate percentages\n",
    "runnable_pct = (runnable_y / total_blocks) * 100\n",
    "output_matches_pct = (correct_y / total_blocks) * 100  # Same as correct for this case\n",
    "incorrect_pct = (correct_n / total_blocks) * 100\n",
    "redundant_pct = (redundant_y / total_blocks) * 100\n",
    "irrelevant_pct = (irrelevant_y / total_blocks) * 100\n",
    "\n",
    "# Correction rate: We fixed the logit_lens bug by using model._model\n",
    "# 3 blocks failed (logit_lens, get_parallelogram_scores, calculate_save_scores)\n",
    "# We demonstrated fixes for all 3\n",
    "failed_blocks = 3\n",
    "corrected_blocks = 3\n",
    "correction_rate_pct = (corrected_blocks / failed_blocks) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"QUANTITATIVE METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Blocks Evaluated: {total_blocks}\")\n",
    "print()\n",
    "print(f\"Runnable%: {runnable_pct:.2f}% ({runnable_y}/{total_blocks})\")\n",
    "print(f\"Output-Matches-Expectation%: {output_matches_pct:.2f}% ({correct_y}/{total_blocks})\")\n",
    "print(f\"Incorrect%: {incorrect_pct:.2f}% ({correct_n}/{total_blocks})\")\n",
    "print(f\"Redundant%: {redundant_pct:.2f}% ({redundant_y}/{total_blocks})\")\n",
    "print(f\"Irrelevant%: {irrelevant_pct:.2f}% ({irrelevant_y}/{total_blocks})\")\n",
    "print(f\"Correction-Rate%: {correction_rate_pct:.2f}% ({corrected_blocks}/{failed_blocks})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store metrics\n",
    "metrics = {\n",
    "    \"Runnable_Percentage\": round(runnable_pct, 2),\n",
    "    \"Output_Matches_Expectation_Percentage\": round(output_matches_pct, 2),\n",
    "    \"Incorrect_Percentage\": round(incorrect_pct, 2),\n",
    "    \"Redundant_Percentage\": round(redundant_pct, 2),\n",
    "    \"Irrelevant_Percentage\": round(irrelevant_pct, 2),\n",
    "    \"Correction_Rate_Percentage\": round(correction_rate_pct, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a3f0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Binary Checklist Summary (C1-C4)\n",
    "\n",
    "Summary of whether any violations exist in the codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52936399",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BINARY CHECKLIST SUMMARY\n",
      "================================================================================\n",
      "                        Checklist_Item                               Condition PASS_FAIL\n",
      "C1: All core analysis code is runnable               No block has Runnable = N      FAIL\n",
      "   C2: All implementations are correct No block has Correct-Implementation = N      FAIL\n",
      "                 C3: No redundant code              No block has Redundant = Y      PASS\n",
      "                C4: No irrelevant code             No block has Irrelevant = Y      PASS\n",
      "================================================================================\n",
      "\n",
      "RATIONALE:\n",
      "--------------------------------------------------------------------------------\n",
      "C1 (FAIL): 3 blocks failed to run due to nnsight model access bug in logit_lens function.\n",
      "C2 (FAIL): 3 blocks have incorrect implementation - they use model.lm_head/model.model.norm\n",
      "    directly instead of model._model.lm_head/model._model.model.norm for nnsight LanguageModel.\n",
      "C3 (PASS): No blocks were found to duplicate computation from other blocks.\n",
      "C4 (PASS): All blocks contribute to the project goal of parallelogram arithmetic analysis.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Binary Checklist Summary\n",
    "print(\"=\" * 80)\n",
    "print(\"BINARY CHECKLIST SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# C1: All core analysis code is runnable\n",
    "c1_pass = runnable_n == 0\n",
    "c1_status = \"PASS\" if c1_pass else \"FAIL\"\n",
    "c1_condition = \"No block has Runnable = N\"\n",
    "\n",
    "# C2: All implementations are correct\n",
    "c2_pass = correct_n == 0\n",
    "c2_status = \"PASS\" if c2_pass else \"FAIL\"\n",
    "c2_condition = \"No block has Correct-Implementation = N\"\n",
    "\n",
    "# C3: No redundant code\n",
    "c3_pass = redundant_y == 0\n",
    "c3_status = \"PASS\" if c3_pass else \"FAIL\"\n",
    "c3_condition = \"No block has Redundant = Y\"\n",
    "\n",
    "# C4: No irrelevant code\n",
    "c4_pass = irrelevant_y == 0\n",
    "c4_status = \"PASS\" if c4_pass else \"FAIL\"\n",
    "c4_condition = \"No block has Irrelevant = Y\"\n",
    "\n",
    "# Create checklist table\n",
    "checklist_data = [\n",
    "    {\"Checklist_Item\": \"C1: All core analysis code is runnable\", \"Condition\": c1_condition, \"PASS_FAIL\": c1_status},\n",
    "    {\"Checklist_Item\": \"C2: All implementations are correct\", \"Condition\": c2_condition, \"PASS_FAIL\": c2_status},\n",
    "    {\"Checklist_Item\": \"C3: No redundant code\", \"Condition\": c3_condition, \"PASS_FAIL\": c3_status},\n",
    "    {\"Checklist_Item\": \"C4: No irrelevant code\", \"Condition\": c4_condition, \"PASS_FAIL\": c4_status},\n",
    "]\n",
    "\n",
    "checklist_df = pd.DataFrame(checklist_data)\n",
    "print(checklist_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Rationale\n",
    "print(\"\\nRATIONALE:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"C1 ({c1_status}): 3 blocks failed to run due to nnsight model access bug in logit_lens function.\")\n",
    "print(f\"C2 ({c2_status}): 3 blocks have incorrect implementation - they use model.lm_head/model.model.norm\")\n",
    "print(\"    directly instead of model._model.lm_head/model._model.model.norm for nnsight LanguageModel.\")\n",
    "print(f\"C3 ({c3_status}): No blocks were found to duplicate computation from other blocks.\")\n",
    "print(f\"C4 ({c4_status}): All blocks contribute to the project goal of parallelogram arithmetic analysis.\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d0707",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Final Summary\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "1. **Bug Identified**: The `logit_lens` function in `parallelograms.py` has a bug when used with nnsight's `LanguageModel`. It attempts to access `model.lm_head` and `model.model.norm` directly, but nnsight wraps the model and requires accessing `model._model` to get the underlying PyTorch model.\n",
    "\n",
    "2. **Affected Functions**:\n",
    "   - `logit_lens` (line 18-20)\n",
    "   - `get_parallelogram_scores` (depends on logit_lens)\n",
    "   - `calculate_save_scores` (depends on get_parallelogram_scores)\n",
    "\n",
    "3. **Fix**: Change line 20 in `parallelograms.py` from:\n",
    "   ```python\n",
    "   return model.lm_head(model.model.norm(concept_vec.cuda())).softmax(dim=-1).detach().cpu()\n",
    "   ```\n",
    "   to:\n",
    "   ```python\n",
    "   return model._model.lm_head(model._model.model.norm(concept_vec.cuda())).softmax(dim=-1).detach().cpu()\n",
    "   ```\n",
    "\n",
    "4. **All other code**: Works correctly and implements the described methodology for parallelogram arithmetic analysis using concept and token induction heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1f68e70",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type bool_ is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Save JSON\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation/code_critic_summary.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_summary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJSON summary saved to: evaluation/code_critic_summary.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mJSON Content:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/json/encoder.py:432\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/json/encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/json/encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/json/encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type bool_ is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# Create and save the JSON summary file\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Change back to eval_agent directory for saving\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "\n",
    "# Create evaluation directory if it doesn't exist\n",
    "os.makedirs('evaluation', exist_ok=True)\n",
    "\n",
    "# Create the JSON summary\n",
    "json_summary = {\n",
    "    \"Runnable_Percentage\": metrics[\"Runnable_Percentage\"],\n",
    "    \"Incorrect_Percentage\": metrics[\"Incorrect_Percentage\"],\n",
    "    \"Redundant_Percentage\": metrics[\"Redundant_Percentage\"],\n",
    "    \"Irrelevant_Percentage\": metrics[\"Irrelevant_Percentage\"],\n",
    "    \"Correction_Rate_Percentage\": metrics[\"Correction_Rate_Percentage\"],\n",
    "    \n",
    "    \"Issues\": {\n",
    "        \"Runnable_Issues_Exist\": runnable_n > 0,\n",
    "        \"Output_Mismatch_Exists\": correct_n > 0,\n",
    "        \"Incorrect_Exists\": correct_n > 0,\n",
    "        \"Redundant_Exists\": redundant_y > 0,\n",
    "        \"Irrelevant_Exists\": irrelevant_y > 0\n",
    "    },\n",
    "    \n",
    "    \"Checklist\": {\n",
    "        \"C1_All_Runnable\": c1_status,\n",
    "        \"C2_All_Correct\": c2_status,\n",
    "        \"C3_No_Redundant\": c3_status,\n",
    "        \"C4_No_Irrelevant\": c4_status\n",
    "    },\n",
    "    \n",
    "    \"Rationale\": {\n",
    "        \"C1_All_Runnable\": \"FAIL: 3 blocks (logit_lens, get_parallelogram_scores, calculate_save_scores) failed to run due to nnsight model access bug. The code uses model.lm_head/model.model.norm directly but nnsight LanguageModel requires model._model to access the underlying PyTorch model.\",\n",
    "        \"C2_All_Correct\": \"FAIL: 3 blocks have incorrect implementation. The logit_lens function incorrectly accesses the nnsight-wrapped model, causing AttributeError when executed.\",\n",
    "        \"C3_No_Redundant\": \"PASS: No blocks were found to duplicate computation from other blocks. Each function serves a distinct purpose in the parallelogram arithmetic analysis pipeline.\",\n",
    "        \"C4_No_Irrelevant\": \"PASS: All blocks contribute to the project goal of analyzing concept and token induction heads for word2vec-style parallelogram arithmetic as described in the plan and codewalk.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "with open('evaluation/code_critic_summary.json', 'w') as f:\n",
    "    json.dump(json_summary, f, indent=2)\n",
    "\n",
    "print(\"JSON summary saved to: evaluation/code_critic_summary.json\")\n",
    "print(\"\\nJSON Content:\")\n",
    "print(json.dumps(json_summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50d218eb",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON summary saved to: evaluation/code_critic_summary.json\n",
      "\n",
      "JSON Content:\n",
      "{\n",
      "  \"Runnable_Percentage\": 88.46,\n",
      "  \"Incorrect_Percentage\": 11.54,\n",
      "  \"Redundant_Percentage\": 0.0,\n",
      "  \"Irrelevant_Percentage\": 0.0,\n",
      "  \"Correction_Rate_Percentage\": 100.0,\n",
      "  \"Issues\": {\n",
      "    \"Runnable_Issues_Exist\": true,\n",
      "    \"Output_Mismatch_Exists\": true,\n",
      "    \"Incorrect_Exists\": true,\n",
      "    \"Redundant_Exists\": false,\n",
      "    \"Irrelevant_Exists\": false\n",
      "  },\n",
      "  \"Checklist\": {\n",
      "    \"C1_All_Runnable\": \"FAIL\",\n",
      "    \"C2_All_Correct\": \"FAIL\",\n",
      "    \"C3_No_Redundant\": \"PASS\",\n",
      "    \"C4_No_Irrelevant\": \"PASS\"\n",
      "  },\n",
      "  \"Rationale\": {\n",
      "    \"C1_All_Runnable\": \"FAIL: 3 blocks (logit_lens, get_parallelogram_scores, calculate_save_scores) failed to run due to nnsight model access bug. The code uses model.lm_head/model.model.norm directly but nnsight LanguageModel requires model._model to access the underlying PyTorch model.\",\n",
      "    \"C2_All_Correct\": \"FAIL: 3 blocks have incorrect implementation. The logit_lens function incorrectly accesses the nnsight-wrapped model, causing AttributeError when executed.\",\n",
      "    \"C3_No_Redundant\": \"PASS: No blocks were found to duplicate computation from other blocks. Each function serves a distinct purpose in the parallelogram arithmetic analysis pipeline.\",\n",
      "    \"C4_No_Irrelevant\": \"PASS: All blocks contribute to the project goal of analyzing concept and token induction heads for word2vec-style parallelogram arithmetic as described in the plan and codewalk.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Fix the numpy bool issue by converting to Python bool\n",
    "json_summary = {\n",
    "    \"Runnable_Percentage\": float(metrics[\"Runnable_Percentage\"]),\n",
    "    \"Incorrect_Percentage\": float(metrics[\"Incorrect_Percentage\"]),\n",
    "    \"Redundant_Percentage\": float(metrics[\"Redundant_Percentage\"]),\n",
    "    \"Irrelevant_Percentage\": float(metrics[\"Irrelevant_Percentage\"]),\n",
    "    \"Correction_Rate_Percentage\": float(metrics[\"Correction_Rate_Percentage\"]),\n",
    "    \n",
    "    \"Issues\": {\n",
    "        \"Runnable_Issues_Exist\": bool(runnable_n > 0),\n",
    "        \"Output_Mismatch_Exists\": bool(correct_n > 0),\n",
    "        \"Incorrect_Exists\": bool(correct_n > 0),\n",
    "        \"Redundant_Exists\": bool(redundant_y > 0),\n",
    "        \"Irrelevant_Exists\": bool(irrelevant_y > 0)\n",
    "    },\n",
    "    \n",
    "    \"Checklist\": {\n",
    "        \"C1_All_Runnable\": c1_status,\n",
    "        \"C2_All_Correct\": c2_status,\n",
    "        \"C3_No_Redundant\": c3_status,\n",
    "        \"C4_No_Irrelevant\": c4_status\n",
    "    },\n",
    "    \n",
    "    \"Rationale\": {\n",
    "        \"C1_All_Runnable\": \"FAIL: 3 blocks (logit_lens, get_parallelogram_scores, calculate_save_scores) failed to run due to nnsight model access bug. The code uses model.lm_head/model.model.norm directly but nnsight LanguageModel requires model._model to access the underlying PyTorch model.\",\n",
    "        \"C2_All_Correct\": \"FAIL: 3 blocks have incorrect implementation. The logit_lens function incorrectly accesses the nnsight-wrapped model, causing AttributeError when executed.\",\n",
    "        \"C3_No_Redundant\": \"PASS: No blocks were found to duplicate computation from other blocks. Each function serves a distinct purpose in the parallelogram arithmetic analysis pipeline.\",\n",
    "        \"C4_No_Irrelevant\": \"PASS: All blocks contribute to the project goal of analyzing concept and token induction heads for word2vec-style parallelogram arithmetic as described in the plan and codewalk.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "with open('evaluation/code_critic_summary.json', 'w') as f:\n",
    "    json.dump(json_summary, f, indent=2)\n",
    "\n",
    "print(\"JSON summary saved to: evaluation/code_critic_summary.json\")\n",
    "print(\"\\nJSON Content:\")\n",
    "print(json.dumps(json_summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df516a90",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluation Complete\n",
    "\n",
    "## Output Files Generated:\n",
    "1. **Jupyter Notebook**: `evaluation/code_critic_evaluation.ipynb`\n",
    "2. **JSON Summary**: `evaluation/code_critic_summary.json`\n",
    "\n",
    "## Summary Statistics:\n",
    "- **Total Blocks Evaluated**: 26\n",
    "- **Runnable%**: 88.46%\n",
    "- **Incorrect%**: 11.54%\n",
    "- **Redundant%**: 0.00%\n",
    "- **Irrelevant%**: 0.00%\n",
    "- **Correction-Rate%**: 100.00%\n",
    "\n",
    "## Checklist Results:\n",
    "| Checklist Item | Status |\n",
    "|----------------|--------|\n",
    "| C1: All core analysis code is runnable | FAIL |\n",
    "| C2: All implementations are correct | FAIL |\n",
    "| C3: No redundant code | PASS |\n",
    "| C4: No irrelevant code | PASS |\n",
    "\n",
    "## Root Cause:\n",
    "The `logit_lens` function in `parallelograms.py` has a bug when using nnsight's `LanguageModel`. It accesses `model.lm_head` and `model.model.norm` directly, but nnsight wraps the model and requires `model._model` to access the underlying PyTorch model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-12-23-02-35_CircuitAnalysisEvaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
