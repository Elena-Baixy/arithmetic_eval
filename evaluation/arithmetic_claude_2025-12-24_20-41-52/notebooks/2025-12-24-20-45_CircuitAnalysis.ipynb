{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6785dbb3",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/smallyan/eval_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/smallyan/eval_agent')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a70f7a2",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository contents:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/net/scratch2/smallyan/belief-tracking_eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# List top-level contents\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepository contents:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_path\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m      8\u001b[0m     full_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(repo_path, item)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(full_path):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/net/scratch2/smallyan/belief-tracking_eval'"
     ]
    }
   ],
   "source": [
    "# First, explore the belief-tracking repository structure\n",
    "import os\n",
    "repo_path = '/net/scratch2/smallyan/belief-tracking_eval'\n",
    "\n",
    "# List top-level contents\n",
    "print(\"Repository contents:\")\n",
    "for item in sorted(os.listdir(repo_path)):\n",
    "    full_path = os.path.join(repo_path, item)\n",
    "    if os.path.isdir(full_path):\n",
    "        print(f\"  [DIR] {item}/\")\n",
    "    else:\n",
    "        print(f\"  [FILE] {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7d2b4a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository contents:\n",
      "  [DIR] .git/\n",
      "  [FILE] .gitignore\n",
      "  [FILE] .python-version\n",
      "  [FILE] CodeWalkthrough.md\n",
      "  [DIR] belief-tracking_claude_2025-12-23_22-28-22/\n",
      "  [FILE] causalmodel_novis.png\n",
      "  [DIR] data/\n",
      "  [FILE] documentation.pdf\n",
      "  [DIR] evaluation/\n",
      "  [DIR] notebooks/\n",
      "  [FILE] plan.md\n",
      "  [FILE] pyproject.toml\n",
      "  [DIR] results/\n",
      "  [DIR] scripts/\n",
      "  [DIR] src/\n",
      "  [FILE] uv.lock\n"
     ]
    }
   ],
   "source": [
    "# The directory is named with underscores, not hyphens\n",
    "repo_path = '/net/scratch2/smallyan/belief_tracking_eval'\n",
    "\n",
    "# List top-level contents\n",
    "print(\"Repository contents:\")\n",
    "for item in sorted(os.listdir(repo_path)):\n",
    "    full_path = os.path.join(repo_path, item)\n",
    "    if os.path.isdir(full_path):\n",
    "        print(f\"  [DIR] {item}/\")\n",
    "    else:\n",
    "        print(f\"  [FILE] {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8690759b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PLAN.MD ===\n",
      "# Plan\n",
      "## Objective\n",
      "Analyze how language models internally represent and track beliefs of characters, particularly when those beliefs may differ from reality, using causal mediation and abstraction methods to uncover systematic computational mechanisms.\n",
      "\n",
      "## Hypothesis\n",
      "1. Language models use a lookback mechanism to track beliefs, where reference information is copied to two locations (address and pointer) enabling later retrieval of important information when needed.\n",
      "2. The model assigns ordering IDs to character, object, and state tokens, then binds character-object-state triples together by co-locating their reference information in low-rank subspaces.\n",
      "3. A binding lookback retrieves the correct state OI using character and object OIs, and an answer lookback retrieves the corresponding state token.\n",
      "4. When visibility information is provided, a visibility lookback uses a visibility ID to retrieve information about the observed character and update the observing character's beliefs.\n",
      "\n",
      "## Methodology\n",
      "1. Construct CausalToM dataset with simple stories involving two characters interacting with objects, creating counterfactual pairs for causal analysis. Analyze Llama-3-70B-Instruct and Llama-3.1-405B-Instruct on 80 correctly-answered samples.\n",
      "2. Use causal mediation analysis with interchange interventions to trace information flow from key input tokens to final output across layers.\n",
      "3. Apply causal abstraction to hypothesize a high-level causal model of belief tracking, then align its variables with internal activations using targeted interchange interventions.\n",
      "4. Use Desiderata-based Component Masking to identify low-rank subspaces encoding specific causal variables, measuring alignment via interchange intervention accuracy (IIA).\n",
      "\n",
      "## Experiments\n",
      "### Localizing Answer Payload\n",
      "- What varied: Layers intervened on at final token position with counterfactual swapping character/object/state order and state values\n",
      "- Metric: Interchange intervention accuracy (IIA) measuring whether output changes from original answer to counterfactual answer\n",
      "- Main result: Answer payload (state token value) localizes to final token residual stream after layer 56 with near-perfect IIA\n",
      "\n",
      "### Localizing Answer Pointer\n",
      "- What varied: Layers intervened on at final token with counterfactual pointer referencing different state than original\n",
      "- Metric: IIA measuring whether output changes to new state (neither original nor counterfactual answer)\n",
      "- Main result: Answer pointer information encoded at final token layers 34-52, redirecting to different state when patched\n",
      "\n",
      "### Localizing Binding Address and Payload\n",
      "- What varied: Swap residual vectors at state tokens between original and counterfactual with sentence order swapped\n",
      "- Metric: IIA measuring whether output flips to the other state token in original input\n",
      "- Main result: Strongest alignment occurs between layers 33-38 at state token residual stream containing both address and payload\n",
      "\n",
      "### Localizing Binding Source Reference\n",
      "- What varied: Interchange character and object token residual streams while freezing state token residual stream\n",
      "- Metric: IIA measuring whether output changes to alternate state token\n",
      "- Main result: Source reference (character and object OIs) encoded in character and object tokens layers 20-34\n",
      "\n",
      "### Localizing Visibility Source Reference\n",
      "- What varied: Intervene on visibility sentence tokens swapping visibility from unobserved to observed\n",
      "- Metric: IIA measuring whether output changes from unknown to observed answer\n",
      "- Main result: Visibility ID source encoded in visibility sentence layers 10-23, then splits into address and pointer copies\n",
      "\n",
      "### Localizing Visibility Payload and Address+Pointer\n",
      "- What varied: Intervene on (1) lookback tokens only, (2) both visibility sentence and lookback tokens with visibility flipped counterfactual\n",
      "- Metric: IIA measuring output change from unknown to observed character's answer\n",
      "- Main result: Payload aligns after layer 31 at lookback tokens; combined address+pointer intervention shows alignment layers 24-31 enabling QK-circuit formation\n"
     ]
    }
   ],
   "source": [
    "# Let's read the key documentation files to understand the work\n",
    "import os\n",
    "\n",
    "# Read the plan\n",
    "with open(os.path.join(repo_path, 'plan.md'), 'r') as f:\n",
    "    plan_content = f.read()\n",
    "    \n",
    "print(\"=== PLAN.MD ===\")\n",
    "print(plan_content[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb04e9f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CODEWALKTHROUGH.MD ===\n",
      "# Language Models use Lookbacks to Track Beliefs\n",
      "\n",
      "This repository contains the code and experiments for the paper [\"Language Models use Lookbacks to Track Beliefs\"](https://arxiv.org/abs/2505.14685) by Prakash et al, 2025. The work investigates how language models (specifically Llama-3-70B-Instruct and Llama-3.1-405B-Instruct) represent and track characters' beliefs.\n",
      "\n",
      "\n",
      "![Causal Model in No-Visibility Setting](causalmodel_novis.png)\n",
      "\n",
      "\n",
      "Please check [belief.baulab.info](https://belief.baulab.info/) for more information.\n",
      "\n",
      "## Setup\n",
      "\n",
      "1. Clone the repository:\n",
      "```bash\n",
      "git clone https://github.com/Nix07/belief_tracking.git\n",
      "cd belief_tracking\n",
      "```\n",
      "\n",
      "2. Set up the environment:\n",
      "```bash\n",
      "uv sync\n",
      "source .venv/bin/activate\n",
      "```\n",
      "\n",
      "3. Configure `env.yml` with following environment variables:\n",
      "- Set `NDIF_KEY` for API access\n",
      "- Set `HF_WRITE` for Hugging Face access\n",
      "\n",
      "4. To perform subspace level analysis, you would need singular vectors that you can request by sending an email to [Nikhil](https://nix07.github.io/).\n",
      "\n",
      "## Repository Structure\n",
      "\n",
      "```\n",
      ".\n",
      "‚îú‚îÄ‚îÄ üìä data/                              # Dataset files\n",
      "‚îú‚îÄ‚îÄ üìì notebooks/                         # Jupyter notebooks for experiments\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ attn_knockout/                   # Attention knockout experiments\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ bigToM/                          # BigToM causal model experiments \n",
      "‚îÇ   ‚îú‚îÄ‚îÄ causal_subspace_analysis/        # Causal subspace analysis\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ causalToM_novis/                 # Causal model in no-visibility\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ causalToM_vis/                   # Causal model in explicit visibility   \n",
      "‚îú‚îÄ‚îÄ üìú scripts/                          # Utility scripts\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ patching_scripts/                # Patching experiment scripts\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ tracing_scripts/                 # Causal mediation analysis scripts\n",
      "‚îú‚îÄ‚îÄ üîß src/                              # Source code\n",
      "‚îú‚îÄ‚îÄ üìà results/                          # Experiment results\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ attn_knockout/                   # Attention knockout results\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ bigToM/                          # BigToM experiment results\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ causal_mediation_analysis/       # Tracing experiment results\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ causalToM_novis/                 # No-visibility experiment results\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ causalToM_vis/                   # Visibility experiment results\n",
      "‚îú‚îÄ‚îÄ üìê svd/                              # Singular vector decompositions\n",
      "‚îú‚îÄ‚îÄ üóÇÔ∏è additionals/                      # Additional data and caches\n",
      "‚îî‚îÄ‚îÄ ‚öôÔ∏è env.yml                           # Environment configuration\n",
      "```\n",
      "\n",
      "## Usage\n",
      "\n",
      "The repository contains several components:\n",
      "\n",
      "1. **Dataset**: The `data/` directory contains the CausalToM templates and synthetic entities used to generate samples. Additionally, it also contains BigToM samples. `src/dataset.py` file contains code for generating and processing the CausalToM dataset.\n",
      "\n",
      "2. **Notebooks**: The `notebooks/` directory contains Jupyter notebooks for various experiments investigating the underlying mechanisms. Use notebooks in `notebooks/causalToM_novis` and `notebooks/causalToM_vis` for mechanism exploration. Notebooks do not include subspace intervention experiments. \n",
      "\n",
      "3. **Scripts**: The `scripts/` directory contains utility scripts organized by experiment type:\n",
      "   - `scripts/patching_scripts/`: Contains patching experiment scripts including `run_single_layer_patching_exps.py` and `run_upto_layer_patching_exps.py` to conduct large-scale interchange intervention experiments, including subspace patching.\n",
      "   - `scripts/tracing_scripts/`: Contains causal mediation analysis scripts including `trace.py` for tracing experiments.\n",
      "\n",
      "4. **Results**: The `results/` directory contains experiment outputs organized by experiment type, including attention knockout results, BigToM results, causal mediation analysis results, and CausalToM experiment results.\n",
      "\n",
      "## Citation\n",
      "\n",
      "If you use this code in your research, please cite our paper:\n",
      "\n",
      "```bibtex\n",
      "@misc{prakash2025languagemodelsuselookbacks,\n",
      "      title={Language Models use Lookbacks to Track Beliefs}, \n",
      "      author={Nikhil Prakash and Natalie Shapira and Arnab Sen Sh\n"
     ]
    }
   ],
   "source": [
    "# Read the code walkthrough for more implementation details\n",
    "with open(os.path.join(repo_path, 'CodeWalkthrough.md'), 'r') as f:\n",
    "    walkthrough = f.read()\n",
    "    \n",
    "print(\"=== CODEWALKTHROUGH.MD ===\")\n",
    "print(walkthrough[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6ada1b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory contents:\n",
      "  [DIR] bigtom/\n",
      "  [FILE] story_templates.json\n",
      "  [DIR] synthetic_entities/\n"
     ]
    }
   ],
   "source": [
    "# Look at the data directory to understand the dataset\n",
    "data_path = os.path.join(repo_path, 'data')\n",
    "print(\"Data directory contents:\")\n",
    "for item in sorted(os.listdir(data_path)):\n",
    "    full_path = os.path.join(data_path, item)\n",
    "    if os.path.isdir(full_path):\n",
    "        print(f\"  [DIR] {item}/\")\n",
    "    else:\n",
    "        print(f\"  [FILE] {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb647661",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story templates keys: ['placeholders', 'templates']\n",
      "\n",
      "First few entries:\n",
      "\n",
      "placeholders:\n",
      "{\n",
      "  \"entity\": {\n",
      "    \"character\": [\n",
      "      \"<character_1>\",\n",
      "      \"<character_2>\",\n",
      "      \"<character_3>\",\n",
      "      \"<character_4>\"\n",
      "    ],\n",
      "    \"container\": [\n",
      "      \"<container_1>\",\n",
      "      \"<container_2>\"\n",
      "    ],\n",
      "    \"state\": [\n",
      "      \"<state_1>\",\n",
      "      \"<state_2>\"\n",
      "    ]\n",
      "  },\n",
      "  \"event\": {\n",
      "    \"state_event\": \"<state_event>\",\n",
      "    \"container_event\": \"<container_event>\",\n",
      "    \"state_swap\": \"<state_swap>\"\n",
      "  },\n",
      "  \"question\": {\n",
      "    \"character\": \"<character_q>\",\n",
      "    \"container\": \"<container_q>\",\n",
      "    \"state\": \"<sta\n",
      "\n",
      "templates:\n",
      "[\n",
      "  {\n",
      "    \"context\": \"<character_1> and <character_2> are working in a busy restaurant. To complete an order, <character_1> grabs an opaque <container_1> and fills it with <state_1>. Then <character_2> grabs another opaque <container_2> and fills it with <state_2>. <character_2> cannot observe <character_1>'s actions. <character_1> cannot observe <character_2>'s actions.\",\n",
      "    \"causal_event\": \"A co-worker named <character_2> didn't hear the customer's request and swaps the <state_event> in the <\n"
     ]
    }
   ],
   "source": [
    "# Read the story templates\n",
    "import json\n",
    "\n",
    "with open(os.path.join(data_path, 'story_templates.json'), 'r') as f:\n",
    "    templates = json.load(f)\n",
    "    \n",
    "print(\"Story templates keys:\", list(templates.keys()) if isinstance(templates, dict) else \"List with length \" + str(len(templates)))\n",
    "print(\"\\nFirst few entries:\")\n",
    "if isinstance(templates, dict):\n",
    "    for k, v in list(templates.items())[:3]:\n",
    "        print(f\"\\n{k}:\")\n",
    "        print(v if isinstance(v, str) else json.dumps(v, indent=2)[:500])\n",
    "else:\n",
    "    for t in templates[:3]:\n",
    "        print(t if isinstance(t, str) else json.dumps(t, indent=2)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "911d28de",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results directory contents:\n",
      "  [DIR] attn_knockout/\n",
      "        - firstVisSent.json\n",
      "        - secondSent.json\n",
      "        - secondSent_firstVisSent.json\n",
      "  [DIR] bigToM/\n",
      "        - Meta-Llama-3-70B-Instruct\n",
      "  [DIR] causalToM_novis/\n",
      "        - Meta-Llama-3-70B-Instruct\n",
      "        - Meta-Llama-3.1-405B-Instruct-8bit\n",
      "        - Qwen2.5-14B-Instruct\n",
      "  [DIR] causalToM_vis/\n",
      "        - Meta-Llama-3-70B-Instruct\n",
      "        - Meta-Llama-3.1-405B-Instruct-8bit\n",
      "        - Qwen2.5-14B-Instruct\n",
      "  [DIR] causal_mediation_analysis/\n",
      "        - character.json\n",
      "        - object.json\n",
      "        - state.json\n",
      "  [DIR] model_evaluations/\n",
      "        - Llama-2-13b-hf.json\n",
      "        - Llama-2-13b-hf_vis.json\n",
      "        - Llama-2-7b-hf.json\n",
      "        - Llama-2-7b-hf_vis.json\n",
      "        - Llama-3.1-8B-Instruct.json\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the results directory to understand what experiments were run\n",
    "results_path = os.path.join(repo_path, 'results')\n",
    "print(\"Results directory contents:\")\n",
    "for item in sorted(os.listdir(results_path)):\n",
    "    full_path = os.path.join(results_path, item)\n",
    "    if os.path.isdir(full_path):\n",
    "        print(f\"  [DIR] {item}/\")\n",
    "        # Show subdirs\n",
    "        for subitem in sorted(os.listdir(full_path))[:5]:\n",
    "            print(f\"        - {subitem}\")\n",
    "    else:\n",
    "        print(f\"  [FILE] {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63da704a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source directory contents:\n",
      "  dataset.py\n",
      "  global_utils.py\n",
      "  models.txt\n"
     ]
    }
   ],
   "source": [
    "# Check what's in the src directory to understand the implementation\n",
    "src_path = os.path.join(repo_path, 'src')\n",
    "print(\"Source directory contents:\")\n",
    "for item in sorted(os.listdir(src_path)):\n",
    "    print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f99e8df",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models used:\n",
      "meta-llama/Llama-2-7b-hf\n",
      "Qwen/Qwen2.5-7B\n",
      "Qwen/Qwen2.5-7B-Instruct\n",
      "meta-llama/Meta-Llama-3-8B\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "meta-llama/Llama-3.1-8B\n",
      "meta-llama/Llama-3.1-8B-Instruct\n",
      "meta-llama/Llama-2-13b-hf\n",
      "allenai/OLMo-2-1124-13B-Instruct\n",
      "Qwen/Qwen2.5-14B\n",
      "Qwen/Qwen2.5-14B-Instruct\n",
      "google/gemma-3-27b-it\n",
      "allenai/OLMo-2-0325-32B-Instruct\n",
      "meta-llama/Meta-Llama-3-70B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Read the models.txt to see what models are used\n",
    "with open(os.path.join(src_path, 'models.txt'), 'r') as f:\n",
    "    models_content = f.read()\n",
    "print(\"Models used:\")\n",
    "print(models_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd416c02",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset code (first 3000 chars):\n",
      "import json\n",
      "import os\n",
      "import random\n",
      "from dataclasses import dataclass\n",
      "from typing import Literal\n",
      "\n",
      "from dataclasses_json import DataClassJsonMixin\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "# from src.utils import env_utils\n",
      "\n",
      "# Get the absolute path to the data directory\n",
      "SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
      "PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)\n",
      "STORY_TEMPLATE_PATH = os.path.join(PROJECT_ROOT, \"data\", \"story_templates.json\")\n",
      "\n",
      "with open(STORY_TEMPLATE_PATH, \"r\") as f:\n",
      "    STORY_TEMPLATES = json.load(f)\n",
      "\n",
      "\n",
      "@dataclass(frozen=False)\n",
      "class Sample(DataClassJsonMixin):\n",
      "    template_idx: int\n",
      "    characters: list[str]\n",
      "    objects: list[str]\n",
      "    states: list[str]\n",
      "\n",
      "    story: str | None = None\n",
      "    character_belief: list[dict[str, str]] = None\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if len(self.characters) == 1:\n",
      "            self.characters.append(\"<N/A>\")\n",
      "        # assert len(self.states) == 2 and len(self.containers) == 2 and len(self.characters) == 2\n",
      "        # No Duplicates\n",
      "        assert len(list(set(self.states))) == len(self.states)\n",
      "        assert len(list(set(self.objects))) == len(self.objects)\n",
      "        assert len(list(set(self.characters))) == len(self.characters)\n",
      "\n",
      "        self.set_story()\n",
      "\n",
      "    def __eq__(self, other) -> bool:\n",
      "        assert isinstance(other, Sample)\n",
      "        return self.story == other.story\n",
      "\n",
      "    def set_entity_names(self):\n",
      "        # characters\n",
      "        for i, character in enumerate(self.characters):\n",
      "            self.story = self.story.replace(\n",
      "                STORY_TEMPLATES[\"placeholders\"][\"entity\"][\"character\"][i], character\n",
      "            )\n",
      "\n",
      "        # containers\n",
      "        self.story = self.story.replace(\n",
      "            STORY_TEMPLATES[\"placeholders\"][\"entity\"][\"container\"][0],\n",
      "            self.objects[0],\n",
      "        )\n",
      "        self.story = self.story.replace(\n",
      "            STORY_TEMPLATES[\"placeholders\"][\"entity\"][\"container\"][1],\n",
      "            self.objects[1],\n",
      "        )\n",
      "\n",
      "        # states\n",
      "        self.story = self.story.replace(\n",
      "            STORY_TEMPLATES[\"placeholders\"][\"entity\"][\"state\"][0], self.states[0]\n",
      "        )\n",
      "        self.story = self.story.replace(\n",
      "            STORY_TEMPLATES[\"placeholders\"][\"entity\"][\"state\"][1], self.states[1]\n",
      "        )\n",
      "\n",
      "    def set_story(self):\n",
      "        self.template = STORY_TEMPLATES[\"templates\"][self.template_idx]\n",
      "        self.story = self.template[\"context\"]\n",
      "\n",
      "        # true state\n",
      "        self.world_state = {\n",
      "            self.objects[0]: self.states[0],\n",
      "            self.objects[1]: self.states[1],\n",
      "        }\n",
      "        self.character_belief = [self.world_state.copy(), self.world_state.copy()]\n",
      "\n",
      "        # set the character beliefs\n",
      "        if self.template_idx in [0, 2, 3]:\n",
      "            self.character_belief[0][self.objects[1]] = \"unknown\"\n",
      "            self.character_belief[1][self.objects[0]] = \"unknown\"\n",
      "        elif self.template_idx == 1:\n",
      "            self.character_belief[1][self.objects[0]] = \"unknown\"\n",
      "\n",
      "        # set the common entity names\n",
      "        self.set_entity_names()\n",
      "        assert \"<\" not in self.story \n"
     ]
    }
   ],
   "source": [
    "# Read the dataset.py to understand the dataset generation\n",
    "with open(os.path.join(src_path, 'dataset.py'), 'r') as f:\n",
    "    dataset_code = f.read()\n",
    "print(\"Dataset code (first 3000 chars):\")\n",
    "print(dataset_code[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b3615bc",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebooks directory contents:\n",
      "  [DIR] attn_knockout/\n",
      "        - attn_knockout_exp.ipynb\n",
      "  [DIR] bigToM/\n",
      "        - causalmodel_exps.ipynb\n",
      "        - utils.py\n",
      "  [DIR] causalToM_novis/\n",
      "        - answer_lookback.ipynb\n",
      "        - binding_lookback.ipynb\n",
      "        - utils.py\n",
      "  [DIR] causalToM_vis/\n",
      "        - explicit_visibility_exps.ipynb\n",
      "        - utils.py\n",
      "  [DIR] causal_subspace_analysis/\n",
      "        - lookback.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the notebooks to understand the experimental setup better\n",
    "notebooks_path = os.path.join(repo_path, 'notebooks')\n",
    "print(\"Notebooks directory contents:\")\n",
    "for item in sorted(os.listdir(notebooks_path)):\n",
    "    full_path = os.path.join(notebooks_path, item)\n",
    "    if os.path.isdir(full_path):\n",
    "        print(f\"  [DIR] {item}/\")\n",
    "        for subitem in sorted(os.listdir(full_path)):\n",
    "            print(f\"        - {subitem}\")\n",
    "    else:\n",
    "        print(f\"  [FILE] {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f5c28d8",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation directory contents:\n",
      "  code_critic_evaluation.ipynb\n",
      "  code_critic_summary.json\n",
      "  consistency_evaluation.json\n",
      "  replications\n",
      "  self_matching.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the evaluation directory to see what exists\n",
    "eval_path = os.path.join(repo_path, 'evaluation')\n",
    "print(\"Evaluation directory contents:\")\n",
    "if os.path.exists(eval_path):\n",
    "    for item in sorted(os.listdir(eval_path)):\n",
    "        print(f\"  {item}\")\n",
    "else:\n",
    "    print(\"  (directory does not exist - will create)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "215ffc42",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== answer_lookback.ipynb ===\n",
      "\n",
      "--- Cell 0 (code) ---\n",
      "import json\n",
      "import os\n",
      "import random\n",
      "import sys\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import torch\n",
      "from nnsight import CONFIG, LanguageModel\n",
      "from torch.utils.data import DataLoader\n",
      "from tqdm import tqdm\n",
      "from utils import (\n",
      "    error_detection,\n",
      "    get_answer_lookback_payload,\n",
      "    get_reversed_sent_diff_state_counterfacts,\n",
      ")\n",
      "\n",
      "current_dir = os.getcwd()\n",
      "sys.path.append(os.path.dirname(os.path.dirname(current_dir)))\n",
      "from src import global_utils\n",
      "from src.dataset import Sample, Dataset\n",
      "\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "random.seed(10)\n",
      "\n",
      "CONFIG.APP.REMOTE_LOGGING = False\n",
      "CONFIG.set_default_api_key(global_utils.load_env_var(\"NDIF_KEY\"))\n",
      "os.environ[\"HF_TOKEN\"] = global_utils.load_env_var(\"HF_WRITE\")\n",
      "\n",
      "%load_ext autoreload\n",
      "\n",
      "--- Cell 1 (markdown) ---\n",
      "# Load Characters, Objects, and States\n",
      "\n",
      "--- Cell 2 (code) ---\n",
      "all_characters = json.load(\n",
      "    open(\n",
      "        os.path.join(global_utils.DATA_DIR, \"synthetic_entities\", \"characters.json\"),\n",
      "        \"r\",\n",
      "    )\n",
      ")\n",
      "all_objects = json.load(\n",
      "    open(\n",
      "        os.path.join(global_utils.DATA_DIR, \"synthetic_entities\", \"bottles.json\"),\n",
      "        \"r\",\n",
      "    )\n",
      ")\n",
      "all_states = json.load(\n",
      "    open(\n",
      "        os.path.join(global_utils.DATA_DIR, \"synthetic_entities\", \"drinks.json\"),\n",
      "        \"r\",\n",
      "    )\n",
      ")\n",
      "\n",
      "print(f\"#characters: {len(all_characters)}\")\n",
      "print(f\"#objects: {len(all_objects)}\")\n",
      "print(f\"#states: {len(all_states)}\")\n",
      "\n",
      "--- Cell 3 (markdown) ---\n",
      "# Load Model\n",
      "\n",
      "--- Cell 4 (code) ---\n",
      "is_remote = False\n",
      "\n",
      "if is_remote:\n",
      "    model = LanguageModel(\"meta-llama/Llama-3.1-405B-Instruct\")\n",
      "else:\n",
      "    model = LanguageModel(\n",
      "        \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
      "        device_map=\"auto\",\n",
      "        dtype=torch.float16,\n",
      "        dispatch=True,\n",
      "    )\n",
      "\n",
      "--- Cell 5 (markdown) ---\n",
      "# Evaluating models\n",
      "\n",
      "--- Cell 6 (code) ---\n",
      "n_samples = 100\n",
      "batch_size = 1\n",
      "\n",
      "samples = []\n",
      "for i in range(10):\n",
      "    characters = random.sample(all_characters, 2)\n",
      "    objects = random.sample(all_objects, 2)\n",
      "    states = random.sample(all_states, 2)\n",
      "\n",
      "    samples.append(Sample(2,characters, objects, states))\n",
      "\n",
      "dataset = Dataset(samples)\n",
      "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
      "\n",
      "--- Cell 7 (code) ---\n",
      "correct, total = 0, 0\n",
      "for bi, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
      "    prompt = batch[\"prompt\"][0]\n",
      "    target = batch[\"target\"][0]\n",
      "\n",
      "    with torch.no_grad():\n",
      "        with model.trace(prompt):\n",
      "            pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
      "\n",
      "        # print(model.tokenizer.decode([pred]), target)\n",
      "        if model.tokenizer.decode([pred]).lower().strip() == target.lower().strip():\n",
      "            correct += 1\n",
      "        total += 1\n",
      "\n",
      "        del pred\n",
      "        torch.cuda.empty_cache()\n",
      "\n",
      "acc = round(correct / total, 2)\n",
      "print(f\"Accuracy: {acc}\")\n",
      "\n",
      "\n",
      "\n",
      "--- Cell 8 (code) ---\n",
      "\n",
      "\n",
      "--- Cell 9 (markdown) ---\n",
      "# Pointer (Fig 4 in the paper)\n"
     ]
    }
   ],
   "source": [
    "# Let's look at one of the causalToM notebooks to understand the intervention mechanism\n",
    "import nbformat\n",
    "\n",
    "notebook_path = os.path.join(repo_path, 'notebooks', 'causalToM_novis', 'answer_lookback.ipynb')\n",
    "with open(notebook_path, 'r') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Print the first several cells to understand the methodology\n",
    "print(\"=== answer_lookback.ipynb ===\")\n",
    "for i, cell in enumerate(nb.cells[:10]):\n",
    "    print(f\"\\n--- Cell {i} ({cell.cell_type}) ---\")\n",
    "    if cell.cell_type == 'code':\n",
    "        print(cell.source[:1500] if len(cell.source) > 1500 else cell.source)\n",
    "    else:\n",
    "        print(cell.source[:800] if len(cell.source) > 800 else cell.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79272237",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== More cells from answer_lookback.ipynb ===\n",
      "\n",
      "--- Cell 9 (markdown) ---\n",
      "# Pointer (Fig 4 in the paper)\n",
      "\n",
      "--- Cell 10 (code) ---\n",
      "n_samples = 20\n",
      "batch_size = 1\n",
      "\n",
      "dataset = get_reversed_sent_diff_state_counterfacts(\n",
      "    all_characters,\n",
      "    all_objects,\n",
      "    all_states,\n",
      "    n_samples,\n",
      ")\n",
      "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
      "\n",
      "--- Cell 11 (code) ---\n",
      "idx = 0\n",
      "print(\"COUNTERFACTUAL EXAMPLE\")\n",
      "print(\"=\" * 25)\n",
      "print(dataset[idx][\"counterfactual_prompt\"], dataset[idx][\"counterfactual_ans\"])\n",
      "print()\n",
      "\n",
      "print(\"CLEAN EXAMPLE\")\n",
      "print(\"=\" * 25)\n",
      "print(dataset[idx][\"clean_prompt\"], dataset[idx][\"clean_ans\"])\n",
      "print(f\"\\nTarget: {dataset[idx]['target']}\")\n",
      "\n",
      "--- Cell 12 (code) ---\n",
      "_, errors = error_detection(model, dataloader, is_remote)\n",
      "print(\n",
      "    f\"Dataset size to be used for IIA: {len(dataset) - len(errors)} ({len(errors)} errors)\"\n",
      ")\n",
      "\n",
      "--- Cell 13 (code) ---\n",
      "accs_answer_lookback_pointer = {}\n",
      "patch_layers = (\n",
      "    [i for i in range(0, 30, 10)]\n",
      "    + [i for i in range(32, 62, 2)]\n",
      "    + [i for i in range(70, model.config.num_hidden_layers, 10)]\n",
      ")\n",
      "\n",
      "for layer_idx in patch_layers:\n",
      "    correct, total = 0, 0\n",
      "    for bi, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
      "        if bi in errors:\n",
      "            continue\n",
      "        counterfactual_prompt = batch[\"counterfactual_prompt\"][0]\n",
      "        clean_prompt = batch[\"clean_prompt\"][0]\n",
      "        target = batch[\"target\"][0]\n",
      "\n",
      "        with torch.no_grad():\n",
      "            with model.trace(counterfactual_prompt):\n",
      "                counterfactual_layer_out = (\n",
      "                    model.model.layers[layer_idx].output[0, -1].save()\n",
      "                )\n",
      "\n",
      "            with model.trace(clean_prompt):\n",
      "                model.model.layers[layer_idx].output[0, -1] = counterfactual_layer_out\n",
      "                pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
      "\n",
      "            if model.tokenizer.decode([pred]).lower().strip() == target.lower().strip():\n",
      "                correct += 1\n",
      "            total += 1\n",
      "\n",
      "            del pred\n",
      "            torch.cuda.empty_cache()\n",
      "\n",
      "    acc = round(correct / total, 2)\n",
      "    print(f\"Layer: {layer_idx} | Accuracy: {acc}\")\n",
      "\n",
      "    accs_answer_lookback_pointer[layer_idx] = acc\n",
      "\n",
      "--- Cell 14 (code) ---\n",
      "# Visualize accs_binding_addr_and_payload as a line plot\n",
      "plt.figure(figsize=(7, 3))\n",
      "layers = list(accs_answer_lookback_pointer.keys())\n",
      "accuracies = list(accs_answer_lookback_pointer.values())\n",
      "\n",
      "plt.plot(layers, accuracies, marker=\"o\", linestyle=\"-\", linewidth=2)\n",
      "plt.xlabel(\"Layer\")\n",
      "plt.ylabel(\"IIA\")\n",
      "plt.title(\"Answer Lookback Pointer IIA by Layer\")\n",
      "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
      "plt.xticks(layers)\n",
      "plt.ylim(0, 1.1)\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "--- Cell 15 (markdown) ---\n",
      "# Payload (Fig 4 in the paper)\n",
      "\n",
      "--- Cell 16 (code) ---\n",
      "n_samples = 10\n",
      "batch_size = 1\n",
      "\n",
      "dataset = get_answer_lookback_payload(\n",
      "    all_characters,\n",
      "    all_objects,\n",
      "    all_states,\n",
      "    n_samples,\n",
      ")\n",
      "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
      "\n",
      "--- Cell 17 (code) ---\n",
      "idx = 0\n",
      "print(\"COUNTERFACTUAL EXAMPLE\")\n",
      "print(\"=\" * 25)\n",
      "print(dataset[idx][\"counterfactual_prompt\"], dataset[idx][\"counterfactual_ans\"])\n",
      "print()\n",
      "\n",
      "print(\"CLEAN EXAMPLE\")\n",
      "print(\"=\" * 25)\n",
      "print(dataset[idx][\"clean_prompt\"], dataset[idx][\"clean_ans\"])\n",
      "print(f\"\\nTarget: {dataset[idx]['target']}\")\n",
      "\n",
      "--- Cell 18 (code) ---\n",
      "_, errors = error_detection(model, dataloader, is_remote)\n",
      "print(\n",
      "    f\"Dataset size to be used for IIA: {len(dataset) - len(errors)} ({len(errors)} errors)\"\n",
      ")\n",
      "\n",
      "--- Cell 19 (code) ---\n",
      "accs_answer_lookback_payload = {}\n",
      "patch_layers = [i for i in range(0, 60, 10)] + [\n",
      "    i for i in range(52, model.config.num_hidden_layers, 2)\n",
      "]\n",
      "\n",
      "for layer_idx in patch_layers:\n",
      "    correct, total = 0, 0\n",
      "    for bi, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
      "        if bi in errors:\n",
      "            continue\n",
      "        counterfactual_prompt = batch[\"counterfactual_prompt\"][0]\n",
      "        clean_prompt = batch[\"clean_prompt\"][0]\n",
      "        target = batch[\"target\"][0]\n",
      "\n",
      "        with torch.no_grad():\n",
      "            with model.trace(counterfactual_prompt, remote=is_remote):\n",
      "                counterfactual_layer_out = (\n",
      "                    model.model.layers[layer_idx].output[0, -1].save()\n",
      "                )\n",
      "\n",
      "            with model.trace(clean_prompt, remote=is_remote):\n",
      "                model.model.layers[layer_idx].output[0, -1] = counterfactual_layer_out\n",
      "                pred = model.lm_head.output[0, -1].argmax(dim=-1).save()\n",
      "\n",
      "        if model.tokenizer.decode([pred]).lower().strip() == target.lower().strip():\n",
      "            correct += 1\n",
      "        total += 1\n",
      "\n",
      "        del pred\n",
      "        torch.cuda.empty_cache()\n",
      "\n",
      "    acc = round(correct / total, 2)\n",
      "    print(f\"Layer: {layer_idx} | Accuracy: {acc}\")\n",
      "\n",
      "    accs_answer_lookback_payload[layer_idx] = acc\n"
     ]
    }
   ],
   "source": [
    "# Let's look at more of the answer_lookback notebook to understand the intervention mechanism\n",
    "print(\"=== More cells from answer_lookback.ipynb ===\")\n",
    "for i, cell in enumerate(nb.cells[9:20]):\n",
    "    print(f\"\\n--- Cell {i+9} ({cell.cell_type}) ---\")\n",
    "    if cell.cell_type == 'code':\n",
    "        print(cell.source[:2000] if len(cell.source) > 2000 else cell.source)\n",
    "    else:\n",
    "        print(cell.source[:800] if len(cell.source) > 800 else cell.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebd190f8",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== utils.py ===\n",
      "import os\n",
      "import random\n",
      "import sys\n",
      "\n",
      "import torch\n",
      "from nnsight import LanguageModel\n",
      "from torch.utils.data import DataLoader\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Add project root to path before importing from src\n",
      "project_root = os.path.dirname(\n",
      "    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
      ")\n",
      "sys.path.append(project_root)\n",
      "\n",
      "from src.dataset import Dataset, Sample\n",
      "\n",
      "\n",
      "def error_detection(\n",
      "    model: LanguageModel, dataloader: DataLoader, is_remote: bool = False\n",
      ") -> tuple[float, list]:\n",
      "    \"\"\"\n",
      "    Evaluates model performance and identifies errors by comparing predictions on both clean and counterfactual prompts.\n",
      "\n",
      "    Args:\n",
      "        model: The language model to evaluate\n",
      "        dataloader: DataLoader containing clean and counterfactual prompts\n",
      "        is_remote (bool): Whether to run model inference remotely\n",
      "\n",
      "    Returns:\n",
      "        tuple: (accuracy, list of error indices)\n",
      "    \"\"\"\n",
      "    correct, total = 0, 0\n",
      "    errors = []\n",
      "\n",
      "    for bi, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
      "        clean_prompt = batch[\"clean_prompt\"][0]\n",
      "        counterfactual_prompt = batch[\"counterfactual_prompt\"][0]\n",
      "        clean_target = batch[\"clean_ans\"][0]\n",
      "        counterfactual_target = batch[\"counterfactual_ans\"][0]\n",
      "        clean_target = batch[\"clean_ans\"][0]\n",
      "\n",
      "        with torch.no_grad():\n",
      "            with model.trace(remote=is_remote) as tracer:\n",
      "                with tracer.invoke(clean_prompt):\n",
      "                    clean_pred = (\n",
      "                        model.lm_head.output[0, -1].argmax(dim=-1).item().save()\n",
      "                    )\n",
      "\n",
      "                with tracer.invoke(counterfactual_prompt):\n",
      "                    counterfactual_pred = (\n",
      "                        model.lm_head.output[0, -1].argmax(dim=-1).item().save()\n",
      "                    )\n",
      "\n",
      "        if (\n",
      "            model.tokenizer.decode([clean_pred]).lower().strip() == clean_target\n",
      "            and model.tokenizer.decode([counterfactual_pred]).lower().strip()\n",
      "            == counterfactual_target\n",
      "        ):\n",
      "            correct += 1\n",
      "        else:\n",
      "            errors.append(bi)\n",
      "        total += 1\n",
      "\n",
      "        del clean_pred, counterfactual_pred\n",
      "        torch.cuda.empty_cache()\n",
      "\n",
      "    return correct / total, errors\n",
      "\n",
      "\n",
      "def get_reversed_sentence_counterfacts(\n",
      "    all_characters: list, all_objects: list, all_states: list, n_samples: int\n",
      ") -> list:\n",
      "    \"\"\"\n",
      "    Generates counterfactual samples by reversing the sentences and keeping the other elements the clean.\n",
      "\n",
      "    Args:\n",
      "        all_characters (list): List of available characters\n",
      "        all_objects (list): List of available objects\n",
      "        all_states (list): List of available states\n",
      "        n_samples (int): Number of samples to generate\n",
      "\n",
      "    Returns:\n",
      "        list: List of dictionaries containing clean and counterfactual samples with their configurations\n",
      "    \"\"\"\n",
      "    clean_configs, counterfactual_configs = [], []\n",
      "    samples = []\n",
      "\n",
      "    for idx in range(n_samples):\n",
      "        template_idx = 2\n",
      "        characters = random.sample(all_characters, 2)\n",
      "        objects = random.sample(all_objects, 2)\n",
      "        states = random.sample(all_states, 2)\n",
      "\n",
      "        sample = Sample(\n",
      "            template_idx=template_idx,\n",
      "            characters=characters,\n",
      "            objects=objects,\n",
      "            states=states,\n",
      "        )\n",
      "        clean_configs.append(sample)\n",
      "\n",
      "        # To create the counterfactual config, reverse the order of characters, objects, and states.\n",
      "        sample = Sample(\n",
      "            template_idx=template_idx,\n",
      "            characters=list(reversed(characters)),\n",
      "            objects=list(reversed(objects)),\n",
      "            states=list(reversed(states)),\n",
      "        )\n",
      "        counterfactual_configs.append(sample)\n",
      "\n",
      "    clean_dataset = Dataset(clean_configs)\n",
      "    counterfactual_dataset = Dataset(counterfactual_configs)\n",
      "\n",
      "    for idx in range(n_samples):\n",
      "        random_object_idx = random.choice([0, 1])\n",
      "        clean = clean_dataset.__getitem__(\n",
      "            idx,\n",
      "            set_container=random_object_idx,\n",
      "            set_character=random_object_idx,\n",
      "        )\n",
      "        counterfactual = counterfactual_dataset.__getitem__(\n",
      "            idx,\n",
      "            set_container=1 ^ random_object_idx,\n",
      "            set_character=1 ^ random_object_idx,\n",
      "        )\n",
      "\n",
      "        samples.append(\n",
      "            {\n",
      "                \"clean_characters\": clean[\"characters\"],\n",
      "                \"clean_objects\": clean[\"objects\"],\n",
      "                \"clean_states\": clean[\"states\"],\n",
      "                \"clean_story\": clean[\"story\"],\n",
      "                \"clean_question\": clean[\"question\"],\n",
      "                \"clean_prompt\": clean[\"prompt\"],\n",
      "                \"clean_ans\": clean[\"target\"],\n",
      "                \"counterfactual_characters\": counterfactual[\"characters\"],\n",
      "                \"counterfactual_objects\": counterfactual[\"objects\"],\n",
      "                \"counterfactual_states\": counterfactual[\"states\"],\n",
      "                \"counterfactual_story\": counterfactual[\"story\"],\n",
      "                \"counterfactual_question\": counterfactual[\"question\"],\n",
      "                \"counterfactual_prompt\": counterfactual[\"prompt\"],\n",
      "                \"counterfactual_ans\": c\n"
     ]
    }
   ],
   "source": [
    "# Now let's look at the utils.py to understand how the counterfactual datasets are created\n",
    "utils_path = os.path.join(repo_path, 'notebooks', 'causalToM_novis', 'utils.py')\n",
    "with open(utils_path, 'r') as f:\n",
    "    utils_content = f.read()\n",
    "\n",
    "print(\"=== utils.py ===\")\n",
    "print(utils_content[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "131f63fb",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic entities:\n",
      "  bottles.json\n",
      "  characters.json\n",
      "  drinks.json\n",
      "\n",
      "Number of characters: 103\n",
      "Sample characters: ['Dean', 'Beth', 'Jake', 'Josh', 'Karen', 'Carl', 'Lee', 'Pam', 'Donna', 'Frank']\n",
      "\n",
      "Number of drinks (states): 23\n",
      "Sample drinks: ['water', 'milk', 'tea', 'beer', 'soda', 'juice', 'coffee', 'wine', 'gin', 'rum']\n",
      "\n",
      "Number of bottles (objects): 21\n",
      "Sample bottles: ['jar', 'cup', 'mug', 'glass', 'flute', 'pitcher', 'jug', 'bottle', 'can', 'flask']\n"
     ]
    }
   ],
   "source": [
    "# Check the synthetic entities used in the original dataset\n",
    "synthetic_path = os.path.join(repo_path, 'data', 'synthetic_entities')\n",
    "print(\"Synthetic entities:\")\n",
    "for item in sorted(os.listdir(synthetic_path)):\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "# Read one of them\n",
    "with open(os.path.join(synthetic_path, 'characters.json'), 'r') as f:\n",
    "    characters = json.load(f)\n",
    "print(f\"\\nNumber of characters: {len(characters)}\")\n",
    "print(f\"Sample characters: {characters[:10]}\")\n",
    "\n",
    "with open(os.path.join(synthetic_path, 'drinks.json'), 'r') as f:\n",
    "    drinks = json.load(f)\n",
    "print(f\"\\nNumber of drinks (states): {len(drinks)}\")\n",
    "print(f\"Sample drinks: {drinks[:10]}\")\n",
    "\n",
    "with open(os.path.join(synthetic_path, 'bottles.json'), 'r') as f:\n",
    "    bottles = json.load(f)\n",
    "print(f\"\\nNumber of bottles (objects): {len(bottles)}\")\n",
    "print(f\"Sample bottles: {bottles[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b15fe5d",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalToM novis results for Llama-3-70B:\n",
      "  answer_lookback\n",
      "  binding_lookback\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/net/scratch2/smallyan/belief_tracking_eval/results/causalToM_novis/Meta-Llama-3-70B-Instruct/answer_lookback_payload.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Read one of them\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_novis_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer_lookback_payload.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     payload_results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer lookback payload results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/net/scratch2/smallyan/belief_tracking_eval/results/causalToM_novis/Meta-Llama-3-70B-Instruct/answer_lookback_payload.json'"
     ]
    }
   ],
   "source": [
    "# Now let's look at the existing results to understand the layer findings\n",
    "results_novis_path = os.path.join(repo_path, 'results', 'causalToM_novis', 'Meta-Llama-3-70B-Instruct')\n",
    "print(\"CausalToM novis results for Llama-3-70B:\")\n",
    "for item in sorted(os.listdir(results_novis_path)):\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "# Read one of them\n",
    "with open(os.path.join(results_novis_path, 'answer_lookback_payload.json'), 'r') as f:\n",
    "    payload_results = json.load(f)\n",
    "print(\"\\nAnswer lookback payload results:\")\n",
    "print(json.dumps(payload_results, indent=2)[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6af877ae",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer lookback results:\n",
      "  payload\n",
      "  pointer\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/net/scratch2/smallyan/belief_tracking_eval/results/causalToM_novis/Meta-Llama-3-70B-Instruct/answer_lookback/payload'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Read the results\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(answer_lookback_path)):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswer_lookback_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/meta/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/net/scratch2/smallyan/belief_tracking_eval/results/causalToM_novis/Meta-Llama-3-70B-Instruct/answer_lookback/payload'"
     ]
    }
   ],
   "source": [
    "# Let's check the subdirectory structure\n",
    "answer_lookback_path = os.path.join(results_novis_path, 'answer_lookback')\n",
    "print(\"Answer lookback results:\")\n",
    "for item in sorted(os.listdir(answer_lookback_path)):\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "# Read the results\n",
    "for item in sorted(os.listdir(answer_lookback_path)):\n",
    "    with open(os.path.join(answer_lookback_path, item), 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"\\n{item}:\")\n",
    "    print(json.dumps(data, indent=2)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "198e75df",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DIR] Meta-Llama-3-70B-Instruct/\n",
      "  [DIR] answer_lookback/\n",
      "    [DIR] payload/\n",
      "    [DIR] pointer/\n",
      "  [DIR] binding_lookback/\n",
      "    [DIR] address_and_payload/\n",
      "    [DIR] character_oi/\n",
      "    [DIR] object_oi/\n",
      "    [DIR] pointer_charac_and_object/\n",
      "    [DIR] pointer_character/\n",
      "    [DIR] pointer_object/\n",
      "    [DIR] source_1/\n",
      "    [DIR] source_2/\n",
      "[DIR] Meta-Llama-3.1-405B-Instruct-8bit/\n",
      "  [DIR] answer_lookback/\n",
      "    [DIR] payload/\n",
      "    [DIR] pointer/\n",
      "  [DIR] binding_lookback/\n",
      "    [DIR] address_and_payload/\n",
      "    [DIR] character_oi/\n",
      "    [DIR] object_oi/\n",
      "    [DIR] pointer_character/\n",
      "    [DIR] pointer_object/\n",
      "    [DIR] source_1/\n",
      "    [DIR] source_2/\n",
      "[DIR] Qwen2.5-14B-Instruct/\n",
      "  [DIR] answer_lookback/\n",
      "    [DIR] payload/\n",
      "    [DIR] pointer/\n",
      "  [DIR] binding_lookback/\n",
      "    [DIR] address_and_payload/\n",
      "    [DIR] character_oi/\n",
      "    [DIR] object_oi/\n",
      "    [DIR] pointer_character/\n",
      "    [DIR] pointer_object/\n",
      "    [DIR] source_1/\n",
      "    [DIR] source_2/\n"
     ]
    }
   ],
   "source": [
    "# Recursively explore results directory\n",
    "def explore_dir(path, indent=0):\n",
    "    for item in sorted(os.listdir(path)):\n",
    "        full_path = os.path.join(path, item)\n",
    "        if os.path.isdir(full_path):\n",
    "            print(\"  \" * indent + f\"[DIR] {item}/\")\n",
    "            if indent < 2:  # Limit depth\n",
    "                explore_dir(full_path, indent + 1)\n",
    "        else:\n",
    "            print(\"  \" * indent + f\"{item}\")\n",
    "\n",
    "explore_dir(os.path.join(repo_path, 'results', 'causalToM_novis'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc40c6bc",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload results:\n",
      "  0.json\n",
      "  10.json\n",
      "  20.json\n",
      "  30.json\n",
      "  40.json\n",
      "  50.json\n",
      "  51.json\n",
      "  52.json\n",
      "  53.json\n",
      "  54.json\n",
      "  55.json\n",
      "  56.json\n",
      "  57.json\n",
      "  58.json\n",
      "  59.json\n",
      "  60.json\n",
      "  61.json\n",
      "  62.json\n",
      "  63.json\n",
      "  64.json\n",
      "  65.json\n",
      "  66.json\n",
      "  67.json\n",
      "  68.json\n",
      "  69.json\n",
      "  70.json\n",
      "  71.json\n",
      "  72.json\n",
      "  73.json\n",
      "  74.json\n",
      "  75.json\n",
      "  76.json\n",
      "  77.json\n",
      "  78.json\n",
      "  79.json\n",
      "\n",
      "Contents of 0.json:\n",
      "{\n",
      "  \"full_rank\": {\n",
      "    \"accuracy\": 0.0,\n",
      "    \"rank\": null,\n",
      "    \"metadata\": null\n",
      "  },\n",
      "  \"singular_vector\": {\n",
      "    \"accuracy\": 0.0,\n",
      "    \"rank\": 0.0,\n",
      "    \"metadata\": {\n",
      "      \"training_args\": {\n",
      "        \"learning_rate\": 0.1,\n",
      "        \"n_epochs\": 1,\n",
      "        \"lamb\": 0.1\n",
      "      },\n",
      "      \"mask\": [\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0,\n",
      "        0.0\n"
     ]
    }
   ],
   "source": [
    "# Let's read one of the result files to understand the format\n",
    "payload_path = os.path.join(repo_path, 'results', 'causalToM_novis', 'Meta-Llama-3-70B-Instruct', 'answer_lookback', 'payload')\n",
    "print(\"Payload results:\")\n",
    "for item in sorted(os.listdir(payload_path)):\n",
    "    print(f\"  {item}\")\n",
    "    \n",
    "# Read the first file\n",
    "first_file = os.path.join(payload_path, sorted(os.listdir(payload_path))[0])\n",
    "with open(first_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(f\"\\nContents of {sorted(os.listdir(payload_path))[0]}:\")\n",
    "print(json.dumps(data, indent=2)[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0a1df14",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 56 payload results:\n",
      "  Full rank accuracy: 0.8\n",
      "  Singular vector accuracy: 0.775\n",
      "  Singular vector rank: 25.0\n"
     ]
    }
   ],
   "source": [
    "# Let's look at a higher layer where there's an effect (layer 56 based on the plan)\n",
    "layer_56_path = os.path.join(payload_path, '56.json')\n",
    "with open(layer_56_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(\"Layer 56 payload results:\")\n",
    "print(f\"  Full rank accuracy: {data['full_rank']['accuracy']}\")\n",
    "print(f\"  Singular vector accuracy: {data['singular_vector']['accuracy']}\")\n",
    "print(f\"  Singular vector rank: {data['singular_vector']['rank']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7abaf413",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2.5-14B results:\n",
      "  Layer 0: full_rank=0.0\n",
      "  Layer 10: full_rank=0.0\n",
      "  Layer 12: full_rank=0.0\n",
      "  Layer 14: full_rank=0.0\n",
      "  Layer 16: full_rank=0.0\n",
      "  Layer 18: full_rank=0.0\n",
      "  Layer 2: full_rank=0.0\n",
      "  Layer 20: full_rank=0.0\n",
      "  Layer 22: full_rank=0.0\n",
      "  Layer 24: full_rank=0.0\n",
      "  Layer 26: full_rank=0.0\n",
      "  Layer 28: full_rank=0.025\n",
      "  Layer 30: full_rank=0.075\n",
      "  Layer 32: full_rank=0.0625\n",
      "  Layer 34: full_rank=0.0625\n",
      "  Layer 36: full_rank=0.1875\n",
      "  Layer 38: full_rank=0.2\n",
      "  Layer 4: full_rank=0.0\n",
      "  Layer 40: full_rank=0.3875\n",
      "  Layer 42: full_rank=0.875\n",
      "  Layer 44: full_rank=0.975\n",
      "  Layer 46: full_rank=1.0\n",
      "  Layer 47: full_rank=1.0\n",
      "  Layer 6: full_rank=0.0\n",
      "  Layer 8: full_rank=0.0\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the results for a different model that was already tested - Qwen2.5-14B\n",
    "qwen_results_path = os.path.join(repo_path, 'results', 'causalToM_novis', 'Qwen2.5-14B-Instruct', 'answer_lookback', 'payload')\n",
    "print(\"Qwen2.5-14B results:\")\n",
    "for item in sorted(os.listdir(qwen_results_path)):\n",
    "    file_path = os.path.join(qwen_results_path, item)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    layer = item.replace('.json', '')\n",
    "    print(f\"  Layer {layer}: full_rank={data['full_rank']['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac1c5781",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100 80GB PCIe\n",
      "CUDA memory: 79.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Let's check CUDA availability and set up for evaluation\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4da3c35",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploratory session complete. Key findings identified.\n",
      "\n",
      "Original work uses:\n",
      "- Models: Meta-Llama-3-70B-Instruct, Meta-Llama-3.1-405B-Instruct\n",
      "- Extended to: Qwen2.5-14B-Instruct\n",
      "- Dataset: CausalToM with 103 characters, 21 bottles, 23 drinks\n",
      "- Findings: Layer-specific IIA for answer lookback mechanism\n",
      "\n",
      "For GT1 (New Model): Need a model NOT in the paper - could use Llama-3.1-8B-Instruct or Gemma\n",
      "For GT2 (New Data): Need new character/object/state combinations not in original dataset\n",
      "For GT3 (Method): The causal abstraction method - can test on related tasks\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the generalization evaluation notebook\n",
    "# First, shutdown this exploratory session\n",
    "print(\"Exploratory session complete. Key findings identified.\")\n",
    "print(\"\\nOriginal work uses:\")\n",
    "print(\"- Models: Meta-Llama-3-70B-Instruct, Meta-Llama-3.1-405B-Instruct\")\n",
    "print(\"- Extended to: Qwen2.5-14B-Instruct\")\n",
    "print(\"- Dataset: CausalToM with 103 characters, 21 bottles, 23 drinks\")\n",
    "print(\"- Findings: Layer-specific IIA for answer lookback mechanism\")\n",
    "print(\"\\nFor GT1 (New Model): Need a model NOT in the paper - could use Llama-3.1-8B-Instruct or Gemma\")\n",
    "print(\"For GT2 (New Data): Need new character/object/state combinations not in original dataset\")\n",
    "print(\"For GT3 (Method): The causal abstraction method - can test on related tasks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-12-24-20-45_CircuitAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
